{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082a926f",
   "metadata": {},
   "source": [
    "# Custom OCR Model Training Notebook\n",
    "\n",
    "This notebook demonstrates how to train a custom OCR model using deep learning.\n",
    "We'll build a CNN-LSTM architecture with CTC loss for end-to-end text recognition.\n",
    "\n",
    "## üéØ Learning Objectives:\n",
    "- Understand deep learning-based OCR architecture\n",
    "- Learn about CTC (Connectionist Temporal Classification) loss\n",
    "- Train a custom OCR model from scratch\n",
    "- Evaluate model performance with proper metrics\n",
    "- Compare custom model with pretrained solutions\n",
    "\n",
    "## üèóÔ∏è Model Architecture:\n",
    "1. **CNN Feature Extractor**: Extracts visual features from images\n",
    "2. **LSTM Sequence Processor**: Models sequential dependencies\n",
    "3. **CTC Head**: Handles variable-length sequences without alignment\n",
    "\n",
    "## üìã Prerequisites:\n",
    "- PyTorch and related libraries installed\n",
    "- Training dataset prepared\n",
    "- GPU recommended for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462b2e1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all necessary libraries for deep learning and OCR training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import custom modules\n",
    "from scripts.custom_model import create_model, count_parameters\n",
    "from utils.dataset import CharacterMapping, OCRDataset, ctc_collate_fn, create_sample_dataset\n",
    "from utils.metrics import calculate_detailed_metrics, print_metrics_report, MetricsTracker\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f47b71",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "Let's define our training configuration and set up paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Data paths\n",
    "    'train_csv': project_root / 'data' / 'train' / 'dataset.csv',\n",
    "    'val_csv': project_root / 'data' / 'val' / 'dataset.csv',\n",
    "    'train_dir': project_root / 'data' / 'train',\n",
    "    'val_dir': project_root / 'data' / 'val',\n",
    "    \n",
    "    # Model parameters\n",
    "    'img_height': 32,\n",
    "    'img_width': 128,\n",
    "    'lstm_hidden_size': 256,\n",
    "    'lstm_num_layers': 2,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 16,  # Smaller batch size for notebook\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'optimizer': 'adam',\n",
    "    \n",
    "    # Other parameters\n",
    "    'num_workers': 2,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = {\n",
    "    'models': project_root / 'models',\n",
    "    'checkpoints': project_root / 'models' / 'checkpoints',\n",
    "    'logs': project_root / 'models' / 'logs',\n",
    "    'results': project_root / 'results'\n",
    "}\n",
    "\n",
    "for name, path in output_dirs.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"üìÅ {name.capitalize()} directory: {path}\")\n",
    "\n",
    "device = torch.device(config['device'])\n",
    "print(f\"\\nüîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08505f80",
   "metadata": {},
   "source": [
    "## 3. Create Sample Dataset (if needed)\n",
    "\n",
    "If you don't have a training dataset yet, let's create a sample one for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data exists\n",
    "if not config['train_csv'].exists():\n",
    "    print(\"üìä Creating sample training dataset...\")\n",
    "    create_sample_dataset(str(config['train_dir']), num_samples=200)\n",
    "    print(\"‚úÖ Sample training dataset created!\")\n",
    "else:\n",
    "    print(\"‚úÖ Training dataset found!\")\n",
    "\n",
    "if not config['val_csv'].exists():\n",
    "    print(\"üìä Creating sample validation dataset...\")\n",
    "    create_sample_dataset(str(config['val_dir']), num_samples=50)\n",
    "    print(\"‚úÖ Sample validation dataset created!\")\n",
    "else:\n",
    "    print(\"‚úÖ Validation dataset found!\")\n",
    "\n",
    "# Load and inspect datasets\n",
    "train_df = pd.read_csv(config['train_csv'])\n",
    "val_df = pd.read_csv(config['val_csv'])\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"\\nüìã Sample training data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b6aff",
   "metadata": {},
   "source": [
    "## 4. Initialize Character Mapping and Model\n",
    "\n",
    "Create character mapping and initialize our custom OCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize character mapping\n",
    "char_mapping = CharacterMapping()\n",
    "print(f\"üìù Character mapping created with {char_mapping.num_classes} classes\")\n",
    "print(f\"Characters: {char_mapping.characters}\")\n",
    "\n",
    "# Test character mapping\n",
    "test_text = \"hello world 123\"\n",
    "encoded = char_mapping.encode(test_text)\n",
    "decoded = char_mapping.decode(encoded)\n",
    "print(f\"\\nüß™ Character mapping test:\")\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    num_classes=char_mapping.num_classes,\n",
    "    img_height=config['img_height'],\n",
    "    img_width=config['img_width'],\n",
    "    lstm_hidden_size=config['lstm_hidden_size'],\n",
    "    lstm_num_layers=config['lstm_num_layers']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\nüß† Model created with {count_parameters(model):,} trainable parameters\")\n",
    "\n",
    "# Test model with dummy input\n",
    "dummy_input = torch.randn(2, 1, config['img_height'], config['img_width']).to(device)\n",
    "with torch.no_grad():\n",
    "    dummy_output = model(dummy_input)\n",
    "print(f\"üîç Model test - Input: {dummy_input.shape}, Output: {dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c528559b",
   "metadata": {},
   "source": [
    "## 5. Create Data Loaders\n",
    "\n",
    "Set up data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = OCRDataset(\n",
    "    csv_file=str(config['train_csv']),\n",
    "    image_dir=str(config['train_dir']),\n",
    "    char_mapping=char_mapping,\n",
    "    img_height=config['img_height'],\n",
    "    img_width=config['img_width'],\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_dataset = OCRDataset(\n",
    "    csv_file=str(config['val_csv']),\n",
    "    image_dir=str(config['val_dir']),\n",
    "    char_mapping=char_mapping,\n",
    "    img_height=config['img_height'],\n",
    "    img_width=config['img_width'],\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Data loaders created:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Visualize sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nüîç Sample batch:\")\n",
    "print(f\"Images shape: {sample_batch['images'].shape}\")\n",
    "print(f\"Labels shape: {sample_batch['labels'].shape}\")\n",
    "print(f\"Sample texts: {sample_batch['texts'][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5955af6",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Data\n",
    "\n",
    "Let's look at some sample images from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "images = sample_batch['images']\n",
    "texts = sample_batch['texts']\n",
    "\n",
    "for i in range(min(6, len(images))):\n",
    "    # Convert tensor to numpy and denormalize\n",
    "    img = images[i].squeeze().numpy()\n",
    "    img = (img * 0.5) + 0.5  # Denormalize from [-1,1] to [0,1]\n",
    "    \n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f\"Text: '{texts[i]}'\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Training Images', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Show dataset statistics\n",
    "text_lengths = [len(text) for text in train_df['label']]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(text_lengths, bins=20, alpha=0.7)\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "char_freq = {}\n",
    "for text in train_df['label']:\n",
    "    for char in text.lower():\n",
    "        char_freq[char] = char_freq.get(char, 0) + 1\n",
    "\n",
    "chars = list(char_freq.keys())[:20]  # Show top 20 characters\n",
    "freqs = [char_freq[char] for char in chars]\n",
    "plt.bar(chars, freqs)\n",
    "plt.title('Character Frequency (Top 20)')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0d2a1",
   "metadata": {},
   "source": [
    "## 7. Initialize Training Components\n",
    "\n",
    "Set up optimizer, loss function, and metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=20,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "# Initialize CTC loss function\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# Initialize metrics tracking\n",
    "metrics_tracker = MetricsTracker()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize TensorBoard logging (optional)\n",
    "log_dir = output_dirs['logs'] / f\"experiment_{int(time.time())}\"\n",
    "writer = SummaryWriter(log_dir=str(log_dir))\n",
    "\n",
    "print(f\"üîß Training components initialized:\")\n",
    "print(f\"Optimizer: {config['optimizer']}\")\n",
    "print(f\"Learning rate: {config['learning_rate']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")\n",
    "print(f\"Number of epochs: {config['num_epochs']}\")\n",
    "print(f\"üìä TensorBoard logs: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb6554",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Now let's train our custom OCR model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        label_lengths = batch['label_lengths'].to(device)\n",
    "        input_lengths = batch['input_lengths'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.transpose(0, 1)  # [seq_len, batch_size, num_classes]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Avg': f'{epoch_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "    \n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, char_mapping, device):\n",
    "    \"\"\"Validate model for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch['images'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            label_lengths = batch['label_lengths'].to(device)\n",
    "            input_lengths = batch['input_lengths'].to(device)\n",
    "            texts = batch['texts']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            outputs_for_loss = outputs.transpose(0, 1)\n",
    "            loss = criterion(outputs_for_loss, labels, input_lengths, label_lengths)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model.predict(images)\n",
    "            \n",
    "            # Decode predictions\n",
    "            for i, pred in enumerate(predictions):\n",
    "                pred_text = char_mapping.ctc_decode(pred.cpu().numpy())\n",
    "                target_text = texts[i]\n",
    "                \n",
    "                all_predictions.append(pred_text)\n",
    "                all_targets.append(target_text)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(val_loader)\n",
    "    metrics = calculate_detailed_metrics(all_predictions, all_targets)\n",
    "    \n",
    "    return avg_loss, metrics, all_predictions, all_targets\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"üöÄ Starting training...\")\n",
    "best_val_accuracy = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    # Training phase\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_metrics, val_predictions, val_targets = validate_epoch(\n",
    "        model, val_loader, criterion, char_mapping, device\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_metrics['accuracy'])\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(epoch, val_predictions, val_targets)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Training/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Validation/Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Validation/Accuracy', val_metrics['accuracy'], epoch)\n",
    "    writer.add_scalar('Validation/CER', val_metrics['cer'], epoch)\n",
    "    writer.add_scalar('Validation/WER', val_metrics['wer'], epoch)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['accuracy'] > best_val_accuracy:\n",
    "        best_val_accuracy = val_metrics['accuracy']\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_accuracy': best_val_accuracy,\n",
    "            'config': config,\n",
    "            'char_mapping': char_mapping\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, output_dirs['checkpoints'] / 'best_model.pth')\n",
    "        print(f\"üíæ Saved best model with accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  CER: {val_metrics['cer']:.4f}\")\n",
    "    print(f\"  WER: {val_metrics['wer']:.4f}\")\n",
    "    print(f\"  Best Accuracy: {best_val_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Training completed\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ Training completed in {total_time/60:.2f} minutes\")\n",
    "print(f\"üìä Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d903e",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress\n",
    "\n",
    "Let's plot the training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss plot\n",
    "axes[0, 0].plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "axes[0, 0].plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0, 1].plot(epochs, val_accuracies, 'g-', label='Validation Accuracy')\n",
    "axes[0, 1].set_title('Validation Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning rate plot\n",
    "lr_history = []\n",
    "for epoch in range(len(train_losses)):\n",
    "    if epoch == 0:\n",
    "        lr_history.append(config['learning_rate'])\n",
    "    elif epoch % 20 == 0:\n",
    "        lr_history.append(lr_history[-1] * 0.5)\n",
    "    else:\n",
    "        lr_history.append(lr_history[-1])\n",
    "\n",
    "axes[1, 0].plot(epochs, lr_history, 'm-', label='Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Best metrics summary\n",
    "best_metrics = metrics_tracker.get_best_metrics()\n",
    "metrics_text = f\"\"\"Best Model Performance:\n",
    "Accuracy: {best_metrics['accuracy']:.4f}\n",
    "CER: {best_metrics['cer']:.4f}\n",
    "WER: {best_metrics['wer']:.4f}\n",
    "BLEU: {best_metrics['bleu_score']:.4f}\n",
    "Epoch: {best_metrics['epoch']}\n",
    "\n",
    "Training Statistics:\n",
    "Total Epochs: {len(train_losses)}\n",
    "Training Time: {total_time/60:.1f} min\n",
    "Final Train Loss: {train_losses[-1]:.4f}\n",
    "Final Val Loss: {val_losses[-1]:.4f}\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes, \n",
    "                fontsize=12, verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "axes[1, 1].set_title('Training Summary')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['results'] / 'training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed final metrics\n",
    "print_metrics_report(best_metrics, \"Final Model Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6e0c6",
   "metadata": {},
   "source": [
    "## 10. Test Model Predictions\n",
    "\n",
    "Let's test our trained model on some validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_checkpoint = torch.load(output_dirs['checkpoints'] / 'best_model.pth')\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Get sample batch from validation set\n",
    "sample_batch = next(iter(val_loader))\n",
    "images = sample_batch['images'].to(device)\n",
    "texts = sample_batch['texts']\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model.predict(images)\n",
    "\n",
    "# Decode predictions\n",
    "pred_texts = []\n",
    "for pred in predictions:\n",
    "    pred_text = char_mapping.ctc_decode(pred.cpu().numpy())\n",
    "    pred_texts.append(pred_text)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, len(images))):\n",
    "    # Convert tensor to numpy and denormalize\n",
    "    img = images[i].squeeze().cpu().numpy()\n",
    "    img = (img * 0.5) + 0.5  # Denormalize\n",
    "    \n",
    "    # Check if prediction is correct\n",
    "    is_correct = pred_texts[i].lower().strip() == texts[i].lower().strip()\n",
    "    color = 'green' if is_correct else 'red'\n",
    "    status = '‚úì' if is_correct else '‚úó'\n",
    "    \n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f\"{status} Target: '{texts[i]}'\\nPrediction: '{pred_texts[i]}'\", \n",
    "                     color=color, fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Model Predictions on Validation Data', fontsize=16, y=1.02)\n",
    "plt.savefig(output_dirs['results'] / 'sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy for this batch\n",
    "correct = sum(1 for pred, target in zip(pred_texts, texts) \n",
    "              if pred.lower().strip() == target.lower().strip())\n",
    "batch_accuracy = correct / len(pred_texts)\n",
    "print(f\"\\nüìä Batch accuracy: {batch_accuracy:.4f} ({correct}/{len(pred_texts)})\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(f\"\\nüìù Example predictions:\")\n",
    "for i in range(min(5, len(pred_texts))):\n",
    "    status = '‚úì' if pred_texts[i].lower().strip() == texts[i].lower().strip() else '‚úó'\n",
    "    print(f\"  {status} Target: '{texts[i]}' ‚Üí Prediction: '{pred_texts[i]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2d9ae",
   "metadata": {},
   "source": [
    "## 11. Compare with Pretrained Models\n",
    "\n",
    "Let's compare our custom model with EasyOCR and Pytesseract on the same validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comparison modules (from original OCR project)\n",
    "try:\n",
    "    import easyocr\n",
    "    import pytesseract\n",
    "    from PIL import Image as PILImage\n",
    "    \n",
    "    # Initialize EasyOCR\n",
    "    print(\"üîÑ Initializing EasyOCR...\")\n",
    "    easy_reader = easyocr.Reader(['en'])\n",
    "    \n",
    "    # Get validation images for comparison\n",
    "    val_image_paths = []\n",
    "    val_target_texts = []\n",
    "    \n",
    "    for idx in range(min(20, len(val_dataset))):  # Test on first 20 images\n",
    "        row = val_df.iloc[idx]\n",
    "        image_path = config['val_dir'] / row['imagename']\n",
    "        if image_path.exists():\n",
    "            val_image_paths.append(str(image_path))\n",
    "            val_target_texts.append(row['label'])\n",
    "    \n",
    "    # Get predictions from all three models\n",
    "    print(f\"\\nüîç Comparing models on {len(val_image_paths)} validation images...\")\n",
    "    \n",
    "    # Custom model predictions\n",
    "    custom_predictions = []\n",
    "    for img_path in tqdm(val_image_paths, desc=\"Custom model\"):\n",
    "        # Preprocess image\n",
    "        img = PILImage.open(img_path).convert('L')\n",
    "        img_array = np.array(img)\n",
    "        transformed = train_dataset.transforms(image=img_array)\n",
    "        img_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            pred = model.predict(img_tensor)\n",
    "            pred_text = char_mapping.ctc_decode(pred[0].cpu().numpy())\n",
    "            custom_predictions.append(pred_text)\n",
    "    \n",
    "    # EasyOCR predictions\n",
    "    easyocr_predictions = []\n",
    "    for img_path in tqdm(val_image_paths, desc=\"EasyOCR\"):\n",
    "        try:\n",
    "            results = easy_reader.readtext(img_path)\n",
    "            text = ' '.join([result[1] for result in results])\n",
    "            easyocr_predictions.append(text.strip())\n",
    "        except:\n",
    "            easyocr_predictions.append(\"\")\n",
    "    \n",
    "    # Pytesseract predictions\n",
    "    pytesseract_predictions = []\n",
    "    for img_path in tqdm(val_image_paths, desc=\"Pytesseract\"):\n",
    "        try:\n",
    "            img = PILImage.open(img_path)\n",
    "            text = pytesseract.image_to_string(img, lang='eng')\n",
    "            pytesseract_predictions.append(text.strip())\n",
    "        except:\n",
    "            pytesseract_predictions.append(\"\")\n",
    "    \n",
    "    # Calculate metrics for all models\n",
    "    custom_metrics = calculate_detailed_metrics(custom_predictions, val_target_texts)\n",
    "    easyocr_metrics = calculate_detailed_metrics(easyocr_predictions, val_target_texts)\n",
    "    pytesseract_metrics = calculate_detailed_metrics(pytesseract_predictions, val_target_texts)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': ['Custom CRNN', 'EasyOCR', 'Pytesseract'],\n",
    "        'Accuracy': [custom_metrics['accuracy'], easyocr_metrics['accuracy'], pytesseract_metrics['accuracy']],\n",
    "        'CER': [custom_metrics['cer'], easyocr_metrics['cer'], pytesseract_metrics['cer']],\n",
    "        'WER': [custom_metrics['wer'], easyocr_metrics['wer'], pytesseract_metrics['wer']],\n",
    "        'BLEU': [custom_metrics['bleu_score'], easyocr_metrics['bleu_score'], pytesseract_metrics['bleu_score']]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüèÜ Model Comparison Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics = ['Accuracy', 'CER', 'WER']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = comparison_df[metric]\n",
    "        bars = axes[i].bar(comparison_df['Model'], values, \n",
    "                          color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "        axes[i].set_title(f'{metric} Comparison')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dirs['results'] / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save detailed comparison\n",
    "    detailed_comparison = pd.DataFrame({\n",
    "        'image_path': val_image_paths,\n",
    "        'target': val_target_texts,\n",
    "        'custom_prediction': custom_predictions,\n",
    "        'easyocr_prediction': easyocr_predictions,\n",
    "        'pytesseract_prediction': pytesseract_predictions\n",
    "    })\n",
    "    detailed_comparison.to_csv(output_dirs['results'] / 'detailed_comparison.csv', index=False)\n",
    "    print(f\"\\nüíæ Detailed comparison saved to: {output_dirs['results'] / 'detailed_comparison.csv'}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Cannot compare with pretrained models: {e}\")\n",
    "    print(\"Install easyocr and pytesseract to enable comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e383fa0",
   "metadata": {},
   "source": [
    "## 12. Model Analysis and Insights\n",
    "\n",
    "Let's analyze our model's performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance by text length\n",
    "val_sample_metrics = best_metrics['sample_metrics']\n",
    "\n",
    "# Group by text length\n",
    "length_analysis = {}\n",
    "for sample in val_sample_metrics:\n",
    "    length = len(sample['target'])\n",
    "    if length not in length_analysis:\n",
    "        length_analysis[length] = {'correct': 0, 'total': 0, 'cer_sum': 0}\n",
    "    \n",
    "    length_analysis[length]['total'] += 1\n",
    "    length_analysis[length]['cer_sum'] += sample['cer']\n",
    "    if sample['match']:\n",
    "        length_analysis[length]['correct'] += 1\n",
    "\n",
    "# Calculate accuracy by length\n",
    "lengths = sorted(length_analysis.keys())\n",
    "accuracies_by_length = []\n",
    "cer_by_length = []\n",
    "\n",
    "for length in lengths:\n",
    "    data = length_analysis[length]\n",
    "    accuracy = data['correct'] / data['total']\n",
    "    avg_cer = data['cer_sum'] / data['total']\n",
    "    accuracies_by_length.append(accuracy)\n",
    "    cer_by_length.append(avg_cer)\n",
    "\n",
    "# Plot performance by text length\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(lengths, accuracies_by_length, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Accuracy by Text Length')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(lengths, cer_by_length, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Character Error Rate by Text Length')\n",
    "axes[1].set_xlabel('Text Length (characters)')\n",
    "axes[1].set_ylabel('CER')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dirs['results'] / 'performance_by_length.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Character-level error analysis\n",
    "char_errors = {}\n",
    "char_total = {}\n",
    "\n",
    "for sample in val_sample_metrics:\n",
    "    target = sample['target'].lower()\n",
    "    prediction = sample['prediction'].lower()\n",
    "    \n",
    "    # Simple character-by-character comparison\n",
    "    max_len = max(len(target), len(prediction))\n",
    "    target_padded = target.ljust(max_len)\n",
    "    pred_padded = prediction.ljust(max_len)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        if i < len(target):\n",
    "            char = target[i]\n",
    "            char_total[char] = char_total.get(char, 0) + 1\n",
    "            \n",
    "            if i >= len(prediction) or target[i] != prediction[i]:\n",
    "                char_errors[char] = char_errors.get(char, 0) + 1\n",
    "\n",
    "# Calculate character-level accuracy\n",
    "char_accuracy = {}\n",
    "for char in char_total:\n",
    "    if char_total[char] > 5:  # Only consider characters that appear more than 5 times\n",
    "        errors = char_errors.get(char, 0)\n",
    "        char_accuracy[char] = 1 - (errors / char_total[char])\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_chars = sorted(char_accuracy.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nüìä Character-level Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\n‚ùå Most Difficult Characters:\")\n",
    "for char, acc in sorted_chars[:5]:\n",
    "    print(f\"  '{char}': {acc:.3f} accuracy ({char_errors.get(char, 0)}/{char_total[char]} errors)\")\n",
    "\n",
    "print(\"\\n‚úÖ Most Accurate Characters:\")\n",
    "for char, acc in sorted_chars[-5:]:\n",
    "    print(f\"  '{char}': {acc:.3f} accuracy ({char_errors.get(char, 0)}/{char_total[char]} errors)\")\n",
    "\n",
    "# Model insights and recommendations\n",
    "print(\"\\nüîç Model Insights and Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "avg_accuracy = best_metrics['accuracy']\n",
    "avg_cer = best_metrics['cer']\n",
    "\n",
    "if avg_accuracy > 0.9:\n",
    "    print(\"‚úÖ Excellent performance! Your custom model is working very well.\")\n",
    "elif avg_accuracy > 0.7:\n",
    "    print(\"üëç Good performance! There's room for improvement.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Performance needs improvement. Consider the recommendations below.\")\n",
    "\n",
    "print(f\"\\nüìà Improvement Suggestions:\")\n",
    "if avg_cer > 0.1:\n",
    "    print(\"‚Ä¢ Increase training data size\")\n",
    "    print(\"‚Ä¢ Add more data augmentation\")\n",
    "    print(\"‚Ä¢ Experiment with different model architectures\")\n",
    "\n",
    "if len(lengths) > 1 and max(accuracies_by_length) - min(accuracies_by_length) > 0.3:\n",
    "    print(\"‚Ä¢ Model performance varies significantly with text length\")\n",
    "    print(\"‚Ä¢ Consider training on more balanced text length distribution\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"‚Ä¢ Try transfer learning from pretrained models\")\n",
    "print(\"‚Ä¢ Experiment with attention mechanisms\")\n",
    "print(\"‚Ä¢ Use larger and more diverse datasets\")\n",
    "print(\"‚Ä¢ Implement beam search decoding\")\n",
    "print(\"‚Ä¢ Fine-tune on domain-specific data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d072d9b",
   "metadata": {},
   "source": [
    "## 13. Save Final Model and Summary\n",
    "\n",
    "Let's save our final model and create a comprehensive summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model in multiple formats\n",
    "final_model_path = output_dirs['models'] / 'final_ocr_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'char_mapping': char_mapping,\n",
    "    'training_history': {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    },\n",
    "    'best_metrics': best_metrics,\n",
    "    'total_training_time': total_time\n",
    "}, final_model_path)\n",
    "\n",
    "# Create training summary report\n",
    "summary_report = f\"\"\"# Custom OCR Model Training Summary\n",
    "\n",
    "## Model Configuration\n",
    "- Architecture: CNN + LSTM + CTC\n",
    "- Image Size: {config['img_height']}x{config['img_width']}\n",
    "- LSTM Hidden Size: {config['lstm_hidden_size']}\n",
    "- LSTM Layers: {config['lstm_num_layers']}\n",
    "- Character Classes: {char_mapping.num_classes}\n",
    "- Total Parameters: {count_parameters(model):,}\n",
    "\n",
    "## Training Configuration\n",
    "- Epochs: {config['num_epochs']}\n",
    "- Batch Size: {config['batch_size']}\n",
    "- Learning Rate: {config['learning_rate']}\n",
    "- Optimizer: {config['optimizer']}\n",
    "- Training Samples: {len(train_df)}\n",
    "- Validation Samples: {len(val_df)}\n",
    "\n",
    "## Performance Results\n",
    "- Best Accuracy: {best_metrics['accuracy']:.4f}\n",
    "- Character Error Rate (CER): {best_metrics['cer']:.4f}\n",
    "- Word Error Rate (WER): {best_metrics['wer']:.4f}\n",
    "- BLEU Score: {best_metrics['bleu_score']:.4f}\n",
    "- Best Epoch: {best_metrics['epoch']}\n",
    "\n",
    "## Training Statistics\n",
    "- Total Training Time: {total_time/60:.2f} minutes\n",
    "- Final Training Loss: {train_losses[-1]:.4f}\n",
    "- Final Validation Loss: {val_losses[-1]:.4f}\n",
    "- Device Used: {device}\n",
    "\n",
    "## Model Files\n",
    "- Best Model: models/checkpoints/best_model.pth\n",
    "- Final Model: models/final_ocr_model.pth\n",
    "- Training Logs: {log_dir}\n",
    "- Results: results/\n",
    "\n",
    "## Usage Instructions\n",
    "To use the trained model for inference:\n",
    "\n",
    "```python\n",
    "# Load model\n",
    "checkpoint = torch.load('models/best_model.pth')\n",
    "model = create_model(checkpoint['config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Or use the prediction script\n",
    "python scripts/predict.py --model models/best_model.pth --image path/to/image.jpg\n",
    "```\n",
    "\n",
    "Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "with open(output_dirs['results'] / 'training_summary.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"üíæ Model and summary saved successfully!\")\n",
    "print(f\"üìÑ Training summary: {output_dirs['results'] / 'training_summary.md'}\")\n",
    "print(f\"üß† Final model: {final_model_path}\")\n",
    "print(f\"üèÜ Best model: {output_dirs['checkpoints'] / 'best_model.pth'}\")\n",
    "print(f\"üìä TensorBoard logs: {log_dir}\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETED SUCCESSFULLY! üéâ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Final Results:\")\n",
    "print(f\"   Best Accuracy: {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"   Character Error Rate: {best_metrics['cer']:.4f}\")\n",
    "print(f\"   Word Error Rate: {best_metrics['wer']:.4f}\")\n",
    "print(f\"   Training Time: {total_time/60:.2f} minutes\")\n",
    "print(f\"\\nüöÄ Your custom OCR model is ready for use!\")\n",
    "print(f\"üìÅ Check the results folder for detailed analysis and comparisons.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

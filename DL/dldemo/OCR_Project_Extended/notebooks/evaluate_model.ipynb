{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3c7ffc",
   "metadata": {},
   "source": [
    "# Custom OCR Model Evaluation Notebook\n",
    "\n",
    "This notebook provides comprehensive evaluation and analysis of our trained custom OCR model.\n",
    "We'll test the model's performance, analyze errors, and compare it with other OCR solutions.\n",
    "\n",
    "## ðŸŽ¯ Evaluation Objectives:\n",
    "- Load and test trained custom OCR model\n",
    "- Perform comprehensive error analysis\n",
    "- Compare with pretrained OCR solutions (EasyOCR, Pytesseract)\n",
    "- Analyze performance across different text characteristics\n",
    "- Generate detailed evaluation reports\n",
    "\n",
    "## ðŸ“Š Evaluation Metrics:\n",
    "- **Accuracy**: Exact match percentage\n",
    "- **CER**: Character Error Rate\n",
    "- **WER**: Word Error Rate\n",
    "- **BLEU Score**: Sequence similarity metric\n",
    "- **Confidence Analysis**: Model prediction confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c6e4e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import all necessary libraries for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "import editdistance\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import custom modules\n",
    "from scripts.custom_model import create_model\n",
    "from utils.dataset import CharacterMapping, OCRDataset, ctc_collate_fn\n",
    "from utils.metrics import calculate_detailed_metrics, print_metrics_report\n",
    "from scripts.predict import OCRPredictor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“ Project root: {project_root}\")\n",
    "print(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ’» CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b714c8b",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model and Configuration\n",
    "\n",
    "Load our trained custom OCR model and its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d978df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "model_path = project_root / 'models' / 'checkpoints' / 'best_model.pth'\n",
    "test_csv = project_root / 'data' / 'test' / 'dataset.csv'\n",
    "test_dir = project_root / 'data' / 'test'\n",
    "results_dir = project_root / 'results'\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if model exists\n",
    "if not model_path.exists():\n",
    "    print(f\"âŒ Model not found at {model_path}\")\n",
    "    print(\"Please run the training notebook first to create the model.\")\n",
    "    exit()\n",
    "\n",
    "# Load model checkpoint\n",
    "print(\"ðŸ”„ Loading trained model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Extract configuration and components\n",
    "config = checkpoint['config']\n",
    "char_mapping = checkpoint['char_mapping']\n",
    "best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "training_epoch = checkpoint['epoch']\n",
    "\n",
    "# Create and load model\n",
    "model = create_model(\n",
    "    num_classes=char_mapping.num_classes,\n",
    "    img_height=config['img_height'],\n",
    "    img_width=config['img_width'],\n",
    "    lstm_hidden_size=config['lstm_hidden_size'],\n",
    "    lstm_num_layers=config['lstm_num_layers']\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"ðŸ“Š Model trained for {training_epoch + 1} epochs\")\n",
    "print(f\"ðŸŽ¯ Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"ðŸ§  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"ðŸ”§ Using device: {device}\")\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = OCRPredictor(str(model_path))\n",
    "print(f\"ðŸ”® OCR Predictor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f0ca0",
   "metadata": {},
   "source": [
    "## 3. Load Test Dataset\n",
    "\n",
    "Load test data for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test dataset exists\n",
    "if not test_csv.exists():\n",
    "    print(\"ðŸ“Š Test dataset not found. Using validation dataset for evaluation...\")\n",
    "    test_csv = project_root / 'data' / 'val' / 'dataset.csv'\n",
    "    test_dir = project_root / 'data' / 'val'\n",
    "    \n",
    "    if not test_csv.exists():\n",
    "        print(\"âŒ No evaluation dataset found. Please ensure you have validation or test data.\")\n",
    "        exit()\n",
    "\n",
    "# Load test dataset\n",
    "test_df = pd.read_csv(test_csv)\n",
    "print(f\"ðŸ“Š Loaded test dataset with {len(test_df)} samples\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nðŸ“‹ Sample test data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = OCRDataset(\n",
    "    csv_file=str(test_csv),\n",
    "    image_dir=str(test_dir),\n",
    "    char_mapping=char_mapping,\n",
    "    img_height=config['img_height'],\n",
    "    img_width=config['img_width'],\n",
    "    is_training=False  # No augmentation for evaluation\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"ðŸ”„ Test dataloader created with {len(test_loader)} batches\")\n",
    "\n",
    "# Analyze test dataset characteristics\n",
    "text_lengths = [len(text) for text in test_df['label']]\n",
    "unique_chars = set(''.join(test_df['label'].str.lower()))\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Test Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(test_df)}\")\n",
    "print(f\"  Average text length: {np.mean(text_lengths):.2f} characters\")\n",
    "print(f\"  Min/Max text length: {min(text_lengths)}/{max(text_lengths)} characters\")\n",
    "print(f\"  Unique characters: {len(unique_chars)}\")\n",
    "print(f\"  Character set: {''.join(sorted(unique_chars))[:50]}{'...' if len(unique_chars) > 50 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b406a",
   "metadata": {},
   "source": [
    "## 4. Model Performance Evaluation\n",
    "\n",
    "Evaluate our custom model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation on test dataset\n",
    "print(\"ðŸ” Evaluating model on test dataset...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_confidences = []\n",
    "prediction_times = []\n",
    "detailed_results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "        images = batch['images'].to(device)\n",
    "        texts = batch['texts']\n",
    "        \n",
    "        # Measure prediction time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(images)\n",
    "        predictions = model.predict(images)\n",
    "        \n",
    "        # Calculate confidence scores (using max softmax probability)\n",
    "        probs = F.softmax(outputs, dim=-1)\n",
    "        max_probs = torch.max(probs, dim=-1)[0]\n",
    "        confidences = torch.mean(max_probs, dim=0).cpu().numpy()\n",
    "        \n",
    "        prediction_time = (time.time() - start_time) / len(images)\n",
    "        \n",
    "        # Decode predictions\n",
    "        for i, pred in enumerate(predictions):\n",
    "            pred_text = char_mapping.ctc_decode(pred.cpu().numpy())\n",
    "            target_text = texts[i]\n",
    "            confidence = confidences[i]\n",
    "            \n",
    "            all_predictions.append(pred_text)\n",
    "            all_targets.append(target_text)\n",
    "            all_confidences.append(confidence)\n",
    "            prediction_times.append(prediction_time)\n",
    "            \n",
    "            # Store detailed results\n",
    "            detailed_results.append({\n",
    "                'batch_idx': batch_idx,\n",
    "                'sample_idx': i,\n",
    "                'target': target_text,\n",
    "                'prediction': pred_text,\n",
    "                'confidence': confidence,\n",
    "                'prediction_time': prediction_time,\n",
    "                'correct': pred_text.lower().strip() == target_text.lower().strip()\n",
    "            })\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "test_metrics = calculate_detailed_metrics(all_predictions, all_targets)\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Set Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print_metrics_report(test_metrics, \"Custom OCR Model\")\n",
    "\n",
    "# Additional performance statistics\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "avg_confidence = np.mean(all_confidences)\n",
    "\n",
    "print(f\"\\nâš¡ Performance Statistics:\")\n",
    "print(f\"  Average prediction time: {avg_prediction_time*1000:.2f} ms per image\")\n",
    "print(f\"  Average confidence score: {avg_confidence:.4f}\")\n",
    "print(f\"  Total evaluation time: {sum(prediction_times):.2f} seconds\")\n",
    "print(f\"  Throughput: {len(all_predictions)/sum(prediction_times):.1f} images/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab46291",
   "metadata": {},
   "source": [
    "## 5. Detailed Error Analysis\n",
    "\n",
    "Analyze different types of errors and failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed analysis DataFrame\n",
    "results_df = pd.DataFrame(detailed_results)\n",
    "\n",
    "# Calculate additional metrics for each sample\n",
    "results_df['target_length'] = results_df['target'].apply(len)\n",
    "results_df['prediction_length'] = results_df['prediction'].apply(len)\n",
    "results_df['length_diff'] = results_df['prediction_length'] - results_df['target_length']\n",
    "results_df['edit_distance'] = results_df.apply(\n",
    "    lambda row: editdistance.eval(row['target'].lower(), row['prediction'].lower()), axis=1\n",
    ")\n",
    "results_df['cer'] = results_df['edit_distance'] / results_df['target_length']\n",
    "\n",
    "# Error analysis by text length\n",
    "length_bins = pd.cut(results_df['target_length'], bins=5, labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long'])\n",
    "results_df['length_category'] = length_bins\n",
    "\n",
    "print(\"ðŸ“Š Error Analysis by Text Length:\")\n",
    "length_analysis = results_df.groupby('length_category').agg({\n",
    "    'correct': ['count', 'sum', 'mean'],\n",
    "    'cer': 'mean',\n",
    "    'confidence': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "length_analysis.columns = ['Total', 'Correct', 'Accuracy', 'Avg_CER', 'Avg_Confidence']\n",
    "print(length_analysis)\n",
    "\n",
    "# Confidence vs. Accuracy analysis\n",
    "confidence_bins = pd.cut(results_df['confidence'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "results_df['confidence_category'] = confidence_bins\n",
    "\n",
    "print(f\"\\nðŸ“Š Error Analysis by Confidence:\")\n",
    "confidence_analysis = results_df.groupby('confidence_category').agg({\n",
    "    'correct': ['count', 'sum', 'mean'],\n",
    "    'cer': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "confidence_analysis.columns = ['Total', 'Correct', 'Accuracy', 'Avg_CER']\n",
    "print(confidence_analysis)\n",
    "\n",
    "# Character-level error analysis\n",
    "char_errors = defaultdict(int)\n",
    "char_substitutions = defaultdict(lambda: defaultdict(int))\n",
    "char_total = defaultdict(int)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    target = row['target'].lower()\n",
    "    prediction = row['prediction'].lower()\n",
    "    \n",
    "    # Count character occurrences and errors\n",
    "    for char in target:\n",
    "        char_total[char] += 1\n",
    "    \n",
    "    # Simple character alignment for error analysis\n",
    "    min_len = min(len(target), len(prediction))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if target[i] != prediction[i]:\n",
    "            char_errors[target[i]] += 1\n",
    "            char_substitutions[target[i]][prediction[i]] += 1\n",
    "    \n",
    "    # Handle length differences\n",
    "    if len(target) > len(prediction):\n",
    "        for i in range(min_len, len(target)):\n",
    "            char_errors[target[i]] += 1\n",
    "\n",
    "# Most problematic characters\n",
    "char_error_rates = {}\n",
    "for char in char_total:\n",
    "    if char_total[char] >= 10:  # Only consider frequently occurring characters\n",
    "        error_rate = char_errors[char] / char_total[char]\n",
    "        char_error_rates[char] = error_rate\n",
    "\n",
    "# Sort by error rate\n",
    "sorted_char_errors = sorted(char_error_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nâŒ Most Problematic Characters (Error Rate):\")\n",
    "for char, error_rate in sorted_char_errors[:10]:\n",
    "    total_count = char_total[char]\n",
    "    error_count = char_errors[char]\n",
    "    print(f\"  '{char}': {error_rate:.3f} ({error_count}/{total_count})\")\n",
    "\n",
    "# Most common character substitutions\n",
    "print(f\"\\nðŸ”„ Most Common Character Substitutions:\")\n",
    "all_substitutions = []\n",
    "for source_char, targets in char_substitutions.items():\n",
    "    for target_char, count in targets.items():\n",
    "        all_substitutions.append((f\"'{source_char}' â†’ '{target_char}'\", count))\n",
    "\n",
    "all_substitutions.sort(key=lambda x: x[1], reverse=True)\n",
    "for substitution, count in all_substitutions[:10]:\n",
    "    print(f\"  {substitution}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ed231",
   "metadata": {},
   "source": [
    "## 6. Visualization of Results\n",
    "\n",
    "Create comprehensive visualizations of the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "# 1. Accuracy by text length\n",
    "length_acc = results_df.groupby('length_category')['correct'].mean()\n",
    "axes[0, 0].bar(range(len(length_acc)), length_acc.values, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_xticks(range(len(length_acc)))\n",
    "axes[0, 0].set_xticklabels(length_acc.index, rotation=45)\n",
    "axes[0, 0].set_title('Accuracy by Text Length Category')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(length_acc.values):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Confidence vs Accuracy\n",
    "axes[0, 1].scatter(results_df['confidence'], results_df['correct'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Confidence Score')\n",
    "axes[0, 1].set_ylabel('Correct (1) / Incorrect (0)')\n",
    "axes[0, 1].set_title('Confidence vs Accuracy')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(results_df['confidence'], results_df['correct'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 1].plot(results_df['confidence'], p(results_df['confidence']), \"r--\", alpha=0.8)\n",
    "\n",
    "# 3. Distribution of prediction times\n",
    "axes[1, 0].hist(np.array(prediction_times) * 1000, bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_xlabel('Prediction Time (ms)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Prediction Times')\n",
    "axes[1, 0].axvline(avg_prediction_time * 1000, color='red', linestyle='--', \n",
    "                   label=f'Mean: {avg_prediction_time*1000:.1f} ms')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. CER distribution\n",
    "axes[1, 1].hist(results_df['cer'], bins=30, alpha=0.7, color='orange')\n",
    "axes[1, 1].set_xlabel('Character Error Rate')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Character Error Rates')\n",
    "axes[1, 1].axvline(results_df['cer'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {results_df[\"cer\"].mean():.3f}')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Length difference analysis\n",
    "axes[2, 0].hist(results_df['length_diff'], bins=30, alpha=0.7, color='purple')\n",
    "axes[2, 0].set_xlabel('Length Difference (Prediction - Target)')\n",
    "axes[2, 0].set_ylabel('Frequency')\n",
    "axes[2, 0].set_title('Distribution of Length Differences')\n",
    "axes[2, 0].axvline(0, color='red', linestyle='--', label='Perfect Length Match')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Character error rates visualization\n",
    "if sorted_char_errors:\n",
    "    chars, error_rates = zip(*sorted_char_errors[:15])  # Top 15 problematic characters\n",
    "    axes[2, 1].bar(range(len(chars)), error_rates, alpha=0.7, color='red')\n",
    "    axes[2, 1].set_xticks(range(len(chars)))\n",
    "    axes[2, 1].set_xticklabels([f\"'{c}'\" for c in chars], rotation=45)\n",
    "    axes[2, 1].set_title('Character Error Rates (Top 15)')\n",
    "    axes[2, 1].set_ylabel('Error Rate')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'evaluation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Additional metrics summary\n",
    "print(f\"\\nðŸ“Š Additional Analysis:\")\n",
    "print(f\"  Samples with perfect predictions: {results_df['correct'].sum()}/{len(results_df)} ({results_df['correct'].mean():.2%})\")\n",
    "print(f\"  Samples with CER = 0: {(results_df['cer'] == 0).sum()}/{len(results_df)} ({(results_df['cer'] == 0).mean():.2%})\")\n",
    "print(f\"  Samples with CER > 0.5: {(results_df['cer'] > 0.5).sum()}/{len(results_df)} ({(results_df['cer'] > 0.5).mean():.2%})\")\n",
    "print(f\"  Correlation between confidence and accuracy: {np.corrcoef(results_df['confidence'], results_df['correct'])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf8156",
   "metadata": {},
   "source": [
    "## 7. Sample Predictions Visualization\n",
    "\n",
    "Visualize sample predictions including both correct and incorrect cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample predictions for visualization\n",
    "sample_batch = next(iter(test_loader))\n",
    "sample_images = sample_batch['images'][:8]  # First 8 images\n",
    "sample_texts = sample_batch['texts'][:8]\n",
    "\n",
    "# Get predictions for sample images\n",
    "with torch.no_grad():\n",
    "    sample_images_gpu = sample_images.to(device)\n",
    "    sample_predictions = model.predict(sample_images_gpu)\n",
    "    \n",
    "    # Get confidence scores\n",
    "    outputs = model(sample_images_gpu)\n",
    "    probs = F.softmax(outputs, dim=-1)\n",
    "    max_probs = torch.max(probs, dim=-1)[0]\n",
    "    sample_confidences = torch.mean(max_probs, dim=0).cpu().numpy()\n",
    "\n",
    "# Decode predictions\n",
    "sample_pred_texts = []\n",
    "for pred in sample_predictions:\n",
    "    pred_text = char_mapping.ctc_decode(pred.cpu().numpy())\n",
    "    sample_pred_texts.append(pred_text)\n",
    "\n",
    "# Visualize sample predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(sample_images)):\n",
    "    # Convert tensor to numpy and denormalize\n",
    "    img = sample_images[i].squeeze().cpu().numpy()\n",
    "    img = (img * 0.5) + 0.5  # Denormalize from [-1,1] to [0,1]\n",
    "    \n",
    "    # Check if prediction is correct\n",
    "    target = sample_texts[i]\n",
    "    prediction = sample_pred_texts[i]\n",
    "    confidence = sample_confidences[i]\n",
    "    \n",
    "    is_correct = prediction.lower().strip() == target.lower().strip()\n",
    "    color = 'green' if is_correct else 'red'\n",
    "    status = 'âœ“' if is_correct else 'âœ—'\n",
    "    \n",
    "    # Calculate CER for this sample\n",
    "    cer = editdistance.eval(target.lower(), prediction.lower()) / len(target) if len(target) > 0 else 0\n",
    "    \n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    title = f\"{status} Target: '{target}'\\nPred: '{prediction}'\\nConf: {confidence:.3f}, CER: {cer:.3f}\"\n",
    "    axes[i].set_title(title, color=color, fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Model Predictions', fontsize=16, y=1.02)\n",
    "plt.savefig(results_dir / 'sample_predictions_detailed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Show worst predictions\n",
    "worst_predictions = results_df.nlargest(5, 'cer')\n",
    "print(f\"\\nâŒ Worst Predictions (Highest CER):\")\n",
    "print(\"=\" * 70)\n",
    "for idx, row in worst_predictions.iterrows():\n",
    "    print(f\"Target: '{row['target']}'\")\n",
    "    print(f\"Prediction: '{row['prediction']}'\")\n",
    "    print(f\"CER: {row['cer']:.3f}, Confidence: {row['confidence']:.3f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Show best predictions with high confidence\n",
    "best_predictions = results_df[(results_df['correct'] == True) & (results_df['confidence'] > 0.9)].nlargest(5, 'confidence')\n",
    "print(f\"\\nâœ… Best Predictions (High Confidence & Correct):\")\n",
    "print(\"=\" * 70)\n",
    "for idx, row in best_predictions.iterrows():\n",
    "    print(f\"Target: '{row['target']}'\")\n",
    "    print(f\"Prediction: '{row['prediction']}'\")\n",
    "    print(f\"Confidence: {row['confidence']:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3076fbc",
   "metadata": {},
   "source": [
    "## 8. Comparison with Pretrained Models\n",
    "\n",
    "Compare our custom model with EasyOCR and Pytesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73817dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with pretrained models\n",
    "try:\n",
    "    import easyocr\n",
    "    import pytesseract\n",
    "    from PIL import Image as PILImage\n",
    "    \n",
    "    print(\"ðŸ”„ Initializing pretrained OCR models...\")\n",
    "    easy_reader = easyocr.Reader(['en'])\n",
    "    \n",
    "    # Select subset of test images for comparison (to save time)\n",
    "    comparison_indices = range(0, min(30, len(test_df)), 2)  # Every 2nd image, max 15 images\n",
    "    comparison_df = test_df.iloc[comparison_indices].copy()\n",
    "    \n",
    "    print(f\"ðŸ” Comparing models on {len(comparison_df)} test images...\")\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    comparison_results = []\n",
    "    \n",
    "    for idx, row in tqdm(comparison_df.iterrows(), total=len(comparison_df), desc=\"Comparing models\"):\n",
    "        image_path = test_dir / row['imagename']\n",
    "        target_text = row['label']\n",
    "        \n",
    "        if not image_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # Custom model prediction\n",
    "        try:\n",
    "            custom_result = predictor.predict_single(str(image_path))\n",
    "            custom_pred = custom_result['text']\n",
    "            custom_conf = custom_result['confidence']\n",
    "        except:\n",
    "            custom_pred = \"\"\n",
    "            custom_conf = 0.0\n",
    "        \n",
    "        # EasyOCR prediction\n",
    "        try:\n",
    "            easy_results = easy_reader.readtext(str(image_path))\n",
    "            easy_pred = ' '.join([result[1] for result in easy_results])\n",
    "            easy_conf = np.mean([result[2] for result in easy_results]) if easy_results else 0.0\n",
    "        except:\n",
    "            easy_pred = \"\"\n",
    "            easy_conf = 0.0\n",
    "        \n",
    "        # Pytesseract prediction\n",
    "        try:\n",
    "            img_pil = PILImage.open(image_path)\n",
    "            tesseract_pred = pytesseract.image_to_string(img_pil, lang='eng').strip()\n",
    "            tesseract_conf = 0.5  # Pytesseract doesn't provide confidence easily\n",
    "        except:\n",
    "            tesseract_pred = \"\"\n",
    "            tesseract_conf = 0.0\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'image': row['imagename'],\n",
    "            'target': target_text,\n",
    "            'custom_pred': custom_pred,\n",
    "            'custom_conf': custom_conf,\n",
    "            'easy_pred': easy_pred.strip(),\n",
    "            'easy_conf': easy_conf,\n",
    "            'tesseract_pred': tesseract_pred,\n",
    "            'tesseract_conf': tesseract_conf\n",
    "        })\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comp_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Calculate metrics for each model\n",
    "    custom_metrics_comp = calculate_detailed_metrics(comp_df['custom_pred'].tolist(), comp_df['target'].tolist())\n",
    "    easy_metrics_comp = calculate_detailed_metrics(comp_df['easy_pred'].tolist(), comp_df['target'].tolist())\n",
    "    tesseract_metrics_comp = calculate_detailed_metrics(comp_df['tesseract_pred'].tolist(), comp_df['target'].tolist())\n",
    "    \n",
    "    # Create detailed comparison table\n",
    "    comparison_table = pd.DataFrame({\n",
    "        'Model': ['Custom CRNN', 'EasyOCR', 'Pytesseract'],\n",
    "        'Accuracy': [custom_metrics_comp['accuracy'], easy_metrics_comp['accuracy'], tesseract_metrics_comp['accuracy']],\n",
    "        'CER': [custom_metrics_comp['cer'], easy_metrics_comp['cer'], tesseract_metrics_comp['cer']],\n",
    "        'WER': [custom_metrics_comp['wer'], easy_metrics_comp['wer'], tesseract_metrics_comp['wer']],\n",
    "        'BLEU': [custom_metrics_comp['bleu_score'], easy_metrics_comp['bleu_score'], tesseract_metrics_comp['bleu_score']],\n",
    "        'Avg_Confidence': [comp_df['custom_conf'].mean(), comp_df['easy_conf'].mean(), comp_df['tesseract_conf'].mean()]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nðŸ† Model Comparison Results ({len(comp_df)} samples):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_table.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    metrics_to_plot = ['Accuracy', 'CER', 'WER', 'BLEU']\n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[i//2, i%2]\n",
    "        values = comparison_table[metric].values\n",
    "        bars = ax.bar(comparison_table['Model'], values, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_ylabel(metric)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Highlight best performance\n",
    "        best_idx = np.argmax(values) if metric in ['Accuracy', 'BLEU'] else np.argmin(values)\n",
    "        bars[best_idx].set_color('gold')\n",
    "        bars[best_idx].set_edgecolor('black')\n",
    "        bars[best_idx].set_linewidth(2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'model_comparison_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save detailed comparison results\n",
    "    comp_df.to_csv(results_dir / 'detailed_model_comparison.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Detailed comparison saved to: {results_dir / 'detailed_model_comparison.csv'}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    best_accuracy_model = comparison_table.loc[comparison_table['Accuracy'].idxmax(), 'Model']\n",
    "    best_cer_model = comparison_table.loc[comparison_table['CER'].idxmin(), 'Model']\n",
    "    \n",
    "    print(f\"\\nðŸ… Performance Summary:\")\n",
    "    print(f\"  Best Accuracy: {best_accuracy_model} ({comparison_table['Accuracy'].max():.4f})\")\n",
    "    print(f\"  Best CER: {best_cer_model} ({comparison_table['CER'].min():.4f})\")\n",
    "    \n",
    "    # Show some example comparisons\n",
    "    print(f\"\\nðŸ“ Example Comparisons:\")\n",
    "    print(\"=\" * 80)\n",
    "    for i in range(min(3, len(comp_df))):\n",
    "        row = comp_df.iloc[i]\n",
    "        print(f\"Target: '{row['target']}'\")\n",
    "        print(f\"Custom:    '{row['custom_pred']}' (conf: {row['custom_conf']:.3f})\")\n",
    "        print(f\"EasyOCR:   '{row['easy_pred']}' (conf: {row['easy_conf']:.3f})\")\n",
    "        print(f\"Tesseract: '{row['tesseract_pred']}'\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Cannot compare with pretrained models: {e}\")\n",
    "    print(\"Install easyocr and pytesseract to enable comparison:\")\n",
    "    print(\"pip install easyocr pytesseract\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during comparison: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf8c91",
   "metadata": {},
   "source": [
    "## 9. Generate Comprehensive Evaluation Report\n",
    "\n",
    "Create a detailed evaluation report with all findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "report_content = f\"\"\"# Custom OCR Model Evaluation Report\n",
    "\n",
    "**Evaluation Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Model Path**: {model_path}\n",
    "**Test Dataset**: {test_csv}\n",
    "\n",
    "## Model Information\n",
    "- **Architecture**: CNN + LSTM + CTC Loss\n",
    "- **Training Epochs**: {training_epoch + 1}\n",
    "- **Model Parameters**: {sum(p.numel() for p in model.parameters()):,}\n",
    "- **Image Input Size**: {config['img_height']}x{config['img_width']}\n",
    "- **Character Classes**: {char_mapping.num_classes}\n",
    "- **Device**: {device}\n",
    "\n",
    "## Test Dataset Statistics\n",
    "- **Total Samples**: {len(test_df)}\n",
    "- **Average Text Length**: {np.mean([len(text) for text in test_df['label']]):.2f} characters\n",
    "- **Text Length Range**: {min([len(text) for text in test_df['label']])}-{max([len(text) for text in test_df['label']])} characters\n",
    "- **Unique Characters**: {len(set(''.join(test_df['label'].str.lower())))}\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "### Overall Performance\n",
    "- **Accuracy**: {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\n",
    "- **Character Error Rate (CER)**: {test_metrics['cer']:.4f}\n",
    "- **Word Error Rate (WER)**: {test_metrics['wer']:.4f}\n",
    "- **BLEU Score**: {test_metrics['bleu_score']:.4f}\n",
    "\n",
    "### Performance Statistics\n",
    "- **Average Prediction Time**: {avg_prediction_time*1000:.2f} ms per image\n",
    "- **Throughput**: {len(all_predictions)/sum(prediction_times):.1f} images/second\n",
    "- **Average Confidence**: {avg_confidence:.4f}\n",
    "- **Perfect Predictions**: {results_df['correct'].sum()}/{len(results_df)} ({results_df['correct'].mean():.2%})\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "### Performance by Text Length\n",
    "\"\"\"\n",
    "\n",
    "# Add length analysis to report\n",
    "for category in length_analysis.index:\n",
    "    if pd.notna(category):\n",
    "        stats = length_analysis.loc[category]\n",
    "        report_content += f\"- **{category}**: {stats['Accuracy']:.3f} accuracy, {stats['Avg_CER']:.3f} CER ({int(stats['Total'])} samples)\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "### Most Problematic Characters\n",
    "\"\"\"\n",
    "\n",
    "# Add character error analysis\n",
    "for char, error_rate in sorted_char_errors[:10]:\n",
    "    total_count = char_total[char]\n",
    "    error_count = char_errors[char]\n",
    "    report_content += f\"- **'{char}'**: {error_rate:.3f} error rate ({error_count}/{total_count})\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "### Performance Distribution\n",
    "- **Samples with CER = 0**: {(results_df['cer'] == 0).sum()}/{len(results_df)} ({(results_df['cer'] == 0).mean():.2%})\n",
    "- **Samples with CER > 0.5**: {(results_df['cer'] > 0.5).sum()}/{len(results_df)} ({(results_df['cer'] > 0.5).mean():.2%})\n",
    "- **Confidence-Accuracy Correlation**: {np.corrcoef(results_df['confidence'], results_df['correct'])[0,1]:.3f}\n",
    "\n",
    "## Model Comparison\"\"\"\n",
    "\n",
    "# Add comparison if available\n",
    "try:\n",
    "    if 'comparison_table' in locals():\n",
    "        report_content += f\"\"\"\n",
    "\n",
    "### Comparison with Pretrained Models (on {len(comp_df)} samples)\n",
    "| Model | Accuracy | CER | WER | BLEU | Avg Confidence |\n",
    "|-------|----------|-----|-----|------|----------------|\n",
    "\"\"\"\n",
    "        for _, row in comparison_table.iterrows():\n",
    "            report_content += f\"| {row['Model']} | {row['Accuracy']:.4f} | {row['CER']:.4f} | {row['WER']:.4f} | {row['BLEU']:.4f} | {row['Avg_Confidence']:.4f} |\\n\"\n",
    "        \n",
    "        best_overall = comparison_table.loc[comparison_table['Accuracy'].idxmax(), 'Model']\n",
    "        report_content += f\"\\n**Best Overall Performance**: {best_overall}\\n\"\n",
    "except:\n",
    "    report_content += \"\\n\\n*Comparison with pretrained models not performed (libraries not available)*\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "## Insights and Recommendations\n",
    "\n",
    "### Strengths\n",
    "\"\"\"\n",
    "\n",
    "# Add insights based on performance\n",
    "if test_metrics['accuracy'] > 0.9:\n",
    "    report_content += \"- Excellent overall accuracy indicates strong model performance\\n\"\n",
    "if avg_confidence > 0.8:\n",
    "    report_content += \"- High average confidence suggests reliable predictions\\n\"\n",
    "if avg_prediction_time < 0.1:\n",
    "    report_content += \"- Fast inference speed suitable for real-time applications\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "### Areas for Improvement\n",
    "\"\"\"\n",
    "\n",
    "if test_metrics['cer'] > 0.1:\n",
    "    report_content += \"- Character error rate could be improved with more training data or better architecture\\n\"\n",
    "if len(length_analysis) > 1 and (length_analysis['Accuracy'].max() - length_analysis['Accuracy'].min()) > 0.3:\n",
    "    report_content += \"- Performance varies significantly with text length - consider balanced training data\\n\"\n",
    "if np.corrcoef(results_df['confidence'], results_df['correct'])[0,1] < 0.3:\n",
    "    report_content += \"- Low confidence-accuracy correlation suggests need for better confidence calibration\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "### Recommendations\n",
    "1. **Data Augmentation**: Increase training data diversity with synthetic examples\n",
    "2. **Architecture Improvements**: Consider attention mechanisms or transformer architectures\n",
    "3. **Post-processing**: Implement language model-based correction\n",
    "4. **Domain Adaptation**: Fine-tune on specific document types or fonts\n",
    "5. **Ensemble Methods**: Combine multiple models for better accuracy\n",
    "\n",
    "## Files Generated\n",
    "- **Evaluation Report**: `results/evaluation_report.md`\n",
    "- **Detailed Results**: `results/detailed_evaluation_results.csv`\n",
    "- **Analysis Plots**: `results/evaluation_analysis.png`\n",
    "- **Sample Predictions**: `results/sample_predictions_detailed.png`\n",
    "\n",
    "---\n",
    "*Report generated automatically by Custom OCR Evaluation Notebook*\n",
    "\"\"\"\n",
    "\n",
    "# Save evaluation report\n",
    "with open(results_dir / 'evaluation_report.md', 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "# Save detailed results\n",
    "results_df.to_csv(results_dir / 'detailed_evaluation_results.csv', index=False)\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_summary = {\n",
    "    'test_metrics': test_metrics,\n",
    "    'performance_stats': {\n",
    "        'avg_prediction_time_ms': avg_prediction_time * 1000,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'throughput_images_per_sec': len(all_predictions) / sum(prediction_times)\n",
    "    },\n",
    "    'dataset_stats': {\n",
    "        'total_samples': len(test_df),\n",
    "        'avg_text_length': np.mean([len(text) for text in test_df['label']]),\n",
    "        'unique_characters': len(set(''.join(test_df['label'].str.lower())))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_dir / 'metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print(\"ðŸ“Š Evaluation completed successfully!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸ“„ Detailed report: {results_dir / 'evaluation_report.md'}\")\n",
    "print(f\"ðŸ“ˆ Results CSV: {results_dir / 'detailed_evaluation_results.csv'}\")\n",
    "print(f\"ðŸ“Š Metrics JSON: {results_dir / 'metrics_summary.json'}\")\n",
    "print(f\"ðŸ–¼ï¸ Analysis plots: {results_dir / 'evaluation_analysis.png'}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸŽ¯ Final Evaluation Summary:\")\n",
    "print(f\"   Accuracy: {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"   CER: {test_metrics['cer']:.4f}\")\n",
    "print(f\"   WER: {test_metrics['wer']:.4f}\")\n",
    "print(f\"   Average prediction time: {avg_prediction_time*1000:.2f} ms\")\n",
    "print(f\"   Model confidence: {avg_confidence:.4f}\")\n",
    "print(f\"\\nðŸš€ Your custom OCR model evaluation is complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

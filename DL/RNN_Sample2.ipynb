{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love this movie it is wonderful', 'this film is amazing and i enjoyed it', 'great plot and superb acting loved it', 'highly recommended so good watch it', 'excellent storyline i really liked it', 'fantastic film i will watch again', 'outstanding movie had a nice time', 'lovely direction and wonderful script', 'truly awesome film i recommend it', 'incredible experience definitely positive', 'this movie is terrible i hate it', 'boring film do not watch it again', 'i dislike the story it was dull', 'awful direction poor script overall', 'this was a waste of time and money', 'very bad film no redeeming features', 'not recommended the worst movie ever', 'horrible experience i left midway', 'completely disappointing never again', 'i really hated every part of this']\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------\n",
    "# 1) Sample Toy Dataset (20 sentences, 10 pos, 10 neg)\n",
    "# ------------------------------------------------------\n",
    "positive_sentences = [\n",
    "    \"i love this movie it is wonderful\",\n",
    "    \"this film is amazing and i enjoyed it\",\n",
    "    \"great plot and superb acting loved it\",\n",
    "    \"highly recommended so good watch it\",\n",
    "    \"excellent storyline i really liked it\",\n",
    "    \"fantastic film i will watch again\",\n",
    "    \"outstanding movie had a nice time\",\n",
    "    \"lovely direction and wonderful script\",\n",
    "    \"truly awesome film i recommend it\",\n",
    "    \"incredible experience definitely positive\",\n",
    "    \n",
    "]\n",
    "\n",
    "negative_sentences = [\n",
    "    \"this movie is terrible i hate it\",\n",
    "    \"boring film do not watch it again\",\n",
    "    \"i dislike the story it was dull\",\n",
    "    \"awful direction poor script overall\",\n",
    "    \"this was a waste of time and money\",\n",
    "    \"very bad film no redeeming features\",\n",
    "    \"not recommended the worst movie ever\",\n",
    "    \"horrible experience i left midway\",\n",
    "    \"completely disappointing never again\",\n",
    "    \"i really hated every part of this\"\n",
    "]\n",
    "\n",
    "all_sentences = positive_sentences + negative_sentences\n",
    "print(all_sentences)\n",
    "labels = np.array([1]*len(positive_sentences) + [0]*len(negative_sentences))  # 1=pos, 0=neg\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i love this movie it is wonderful', np.int64(1)), ('this film is amazing and i enjoyed it', np.int64(1)), ('great plot and superb acting loved it', np.int64(1)), ('highly recommended so good watch it', np.int64(1)), ('excellent storyline i really liked it', np.int64(1)), ('fantastic film i will watch again', np.int64(1)), ('outstanding movie had a nice time', np.int64(1)), ('lovely direction and wonderful script', np.int64(1)), ('truly awesome film i recommend it', np.int64(1)), ('incredible experience definitely positive', np.int64(1)), ('this movie is terrible i hate it', np.int64(0)), ('boring film do not watch it again', np.int64(0)), ('i dislike the story it was dull', np.int64(0)), ('awful direction poor script overall', np.int64(0)), ('this was a waste of time and money', np.int64(0)), ('very bad film no redeeming features', np.int64(0)), ('not recommended the worst movie ever', np.int64(0)), ('horrible experience i left midway', np.int64(0)), ('completely disappointing never again', np.int64(0)), ('i really hated every part of this', np.int64(0))]\n",
      "[('i love this movie it is wonderful', np.int64(1)), ('not recommended the worst movie ever', np.int64(0)), ('great plot and superb acting loved it', np.int64(1)), ('completely disappointing never again', np.int64(0)), ('truly awesome film i recommend it', np.int64(1)), ('boring film do not watch it again', np.int64(0)), ('fantastic film i will watch again', np.int64(1)), ('i dislike the story it was dull', np.int64(0)), ('excellent storyline i really liked it', np.int64(1)), ('horrible experience i left midway', np.int64(0)), ('awful direction poor script overall', np.int64(0)), ('this was a waste of time and money', np.int64(0)), ('lovely direction and wonderful script', np.int64(1)), ('outstanding movie had a nice time', np.int64(1)), ('this film is amazing and i enjoyed it', np.int64(1)), ('i really hated every part of this', np.int64(0)), ('very bad film no redeeming features', np.int64(0)), ('this movie is terrible i hate it', np.int64(0)), ('highly recommended so good watch it', np.int64(1)), ('incredible experience definitely positive', np.int64(1))]\n",
      "('i love this movie it is wonderful', 'not recommended the worst movie ever', 'great plot and superb acting loved it', 'completely disappointing never again', 'truly awesome film i recommend it', 'boring film do not watch it again', 'fantastic film i will watch again', 'i dislike the story it was dull', 'excellent storyline i really liked it', 'horrible experience i left midway', 'awful direction poor script overall', 'this was a waste of time and money', 'lovely direction and wonderful script', 'outstanding movie had a nice time', 'this film is amazing and i enjoyed it', 'i really hated every part of this', 'very bad film no redeeming features', 'this movie is terrible i hate it', 'highly recommended so good watch it', 'incredible experience definitely positive')\n",
      "(np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1))\n",
      "['i love this movie it is wonderful'\n",
      " 'not recommended the worst movie ever'\n",
      " 'great plot and superb acting loved it'\n",
      " 'completely disappointing never again'\n",
      " 'truly awesome film i recommend it' 'boring film do not watch it again'\n",
      " 'fantastic film i will watch again' 'i dislike the story it was dull'\n",
      " 'excellent storyline i really liked it'\n",
      " 'horrible experience i left midway' 'awful direction poor script overall'\n",
      " 'this was a waste of time and money'\n",
      " 'lovely direction and wonderful script'\n",
      " 'outstanding movie had a nice time'\n",
      " 'this film is amazing and i enjoyed it'\n",
      " 'i really hated every part of this' 'very bad film no redeeming features'\n",
      " 'this movie is terrible i hate it' 'highly recommended so good watch it'\n",
      " 'incredible experience definitely positive']\n",
      "[1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) Shuffle & Split the Data\n",
    "# ------------------------------------------------------\n",
    "# Combine text + labels, shuffle, then split\n",
    "combined = list(zip(all_sentences, labels))\n",
    "\n",
    "print(combined)\n",
    "random.shuffle(combined)\n",
    "print(combined)\n",
    "shuffled_sentences, shuffled_labels = zip(*combined)\n",
    "print(shuffled_sentences)\n",
    "print(shuffled_labels)\n",
    "\n",
    "shuffled_sentences = np.array(shuffled_sentences)\n",
    "shuffled_labels = np.array(shuffled_labels)\n",
    "print(shuffled_sentences)\n",
    "print(shuffled_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's do an 80/20 split for train vs test\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
    "    shuffled_sentences, shuffled_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['excellent storyline i really liked it',\n",
       "       'boring film do not watch it again',\n",
       "       'this was a waste of time and money',\n",
       "       'completely disappointing never again',\n",
       "       'highly recommended so good watch it',\n",
       "       'very bad film no redeeming features',\n",
       "       'outstanding movie had a nice time',\n",
       "       'great plot and superb acting loved it',\n",
       "       'horrible experience i left midway',\n",
       "       'incredible experience definitely positive',\n",
       "       'truly awesome film i recommend it',\n",
       "       'lovely direction and wonderful script',\n",
       "       'i dislike the story it was dull',\n",
       "       'awful direction poor script overall',\n",
       "       'this film is amazing and i enjoyed it',\n",
       "       'fantastic film i will watch again'], dtype='<U41')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i love this movie it is wonderful',\n",
       "       'this movie is terrible i hate it',\n",
       "       'i really hated every part of this',\n",
       "       'not recommended the worst movie ever'], dtype='<U41')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll further split out a validation set from the training data, 75/25\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    train_sentences, train_labels, \n",
    "    test_size=0.25,  # 0.25 of the training => 0.20 * 0.25 = total 5 test, 3 val, 12 train\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['awful direction poor script overall',\n",
       "       'lovely direction and wonderful script',\n",
       "       'horrible experience i left midway',\n",
       "       'incredible experience definitely positive',\n",
       "       'this was a waste of time and money',\n",
       "       'fantastic film i will watch again',\n",
       "       'highly recommended so good watch it',\n",
       "       'great plot and superb acting loved it',\n",
       "       'truly awesome film i recommend it',\n",
       "       'i dislike the story it was dull',\n",
       "       'completely disappointing never again',\n",
       "       'outstanding movie had a nice time'], dtype='<U41')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['excellent storyline i really liked it',\n",
       "       'boring film do not watch it again',\n",
       "       'very bad film no redeeming features',\n",
       "       'this film is amazing and i enjoyed it'], dtype='<U41')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'fantastic ' in training: fantastic film i will watch again\n"
     ]
    }
   ],
   "source": [
    "for sent in train_sentences:\n",
    "    if \"fantastic\" in sent:\n",
    "        print(\"Found 'fantastic ' in training:\", sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 1, 'i': 2, 'it': 3, 'and': 4, 'direction': 5, 'script': 6, 'experience': 7, 'was': 8, 'a': 9, 'time': 10, 'film': 11, 'watch': 12, 'again': 13, 'awful': 14, 'poor': 15, 'overall': 16, 'lovely': 17, 'wonderful': 18, 'horrible': 19, 'left': 20, 'midway': 21, 'incredible': 22, 'definitely': 23, 'positive': 24, 'this': 25, 'waste': 26, 'of': 27, 'money': 28, 'fantastic': 29, 'will': 30, 'highly': 31, 'recommended': 32, 'so': 33, 'good': 34, 'great': 35, 'plot': 36, 'superb': 37, 'acting': 38, 'loved': 39, 'truly': 40, 'awesome': 41, 'recommend': 42, 'dislike': 43, 'the': 44, 'story': 45, 'dull': 46, 'completely': 47, 'disappointing': 48, 'never': 49, 'outstanding': 50, 'movie': 51, 'had': 52, 'nice': 53}\n",
      "[[25, 1, 9, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) Tokenize & Pad Sequences\n",
    "# ------------------------------------------------------\n",
    "# Hyperparameters for tokenizing\n",
    "vocab_size = 5000\n",
    "max_length = 10  # maximum words in a sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.texts_to_sequences([\"this is a test sentence\"]))\n",
    "# ------------------------------------------------------\n",
    "# 4) Pad Sequences\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def tokenize_and_pad(texts, tokenizer, max_len):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "    return padded\n",
    "\n",
    "train_padded = tokenize_and_pad(train_sentences, tokenizer, max_length)\n",
    "val_padded   = tokenize_and_pad(val_sentences,   tokenizer, max_length)\n",
    "test_padded  = tokenize_and_pad(test_sentences,  tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) Build the LSTM Model\n",
    "# ------------------------------------------------------\n",
    "embedding_dim = 16\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, \n",
    "              output_dim=embedding_dim, \n",
    "              input_length=max_length),\n",
    "    LSTM(64),  # You can change the LSTM units to experiment\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6/6 - 8s - 1s/step - accuracy: 0.4167 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6937\n",
      "Epoch 2/30\n",
      "6/6 - 0s - 36ms/step - accuracy: 0.5833 - loss: 0.6913 - val_accuracy: 0.5000 - val_loss: 0.6945\n",
      "Epoch 3/30\n",
      "6/6 - 0s - 56ms/step - accuracy: 0.5833 - loss: 0.6866 - val_accuracy: 0.5000 - val_loss: 0.6951\n",
      "Epoch 4/30\n",
      "6/6 - 0s - 30ms/step - accuracy: 0.5833 - loss: 0.6849 - val_accuracy: 0.5000 - val_loss: 0.6968\n",
      "Epoch 5/30\n",
      "6/6 - 0s - 34ms/step - accuracy: 0.5833 - loss: 0.6778 - val_accuracy: 0.5000 - val_loss: 0.6995\n",
      "Epoch 6/30\n",
      "6/6 - 0s - 40ms/step - accuracy: 0.5833 - loss: 0.6687 - val_accuracy: 0.5000 - val_loss: 0.7031\n",
      "Epoch 7/30\n",
      "6/6 - 0s - 42ms/step - accuracy: 0.5833 - loss: 0.6566 - val_accuracy: 0.5000 - val_loss: 0.7160\n",
      "Epoch 8/30\n",
      "6/6 - 0s - 38ms/step - accuracy: 0.5833 - loss: 0.6279 - val_accuracy: 0.5000 - val_loss: 0.7520\n",
      "Epoch 9/30\n",
      "6/6 - 0s - 38ms/step - accuracy: 0.5833 - loss: 0.5477 - val_accuracy: 0.5000 - val_loss: 0.8638\n",
      "Epoch 10/30\n",
      "6/6 - 0s - 35ms/step - accuracy: 0.7500 - loss: 0.4379 - val_accuracy: 0.5000 - val_loss: 1.6142\n",
      "Epoch 11/30\n",
      "6/6 - 0s - 44ms/step - accuracy: 0.8333 - loss: 0.2423 - val_accuracy: 0.5000 - val_loss: 2.5928\n",
      "Epoch 12/30\n",
      "6/6 - 0s - 28ms/step - accuracy: 1.0000 - loss: 0.1360 - val_accuracy: 0.5000 - val_loss: 3.2709\n",
      "Epoch 13/30\n",
      "6/6 - 0s - 45ms/step - accuracy: 1.0000 - loss: 0.0818 - val_accuracy: 0.5000 - val_loss: 3.6039\n",
      "Epoch 14/30\n",
      "6/6 - 0s - 30ms/step - accuracy: 1.0000 - loss: 0.0511 - val_accuracy: 0.5000 - val_loss: 3.7525\n",
      "Epoch 15/30\n",
      "6/6 - 0s - 44ms/step - accuracy: 1.0000 - loss: 0.0309 - val_accuracy: 0.2500 - val_loss: 3.7944\n",
      "Epoch 16/30\n",
      "6/6 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.0202 - val_accuracy: 0.0000e+00 - val_loss: 3.7698\n",
      "Epoch 17/30\n",
      "6/6 - 0s - 29ms/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 0.0000e+00 - val_loss: 3.7317\n",
      "Epoch 18/30\n",
      "6/6 - 0s - 29ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.0000e+00 - val_loss: 3.7150\n",
      "Epoch 19/30\n",
      "6/6 - 0s - 43ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.0000e+00 - val_loss: 3.7212\n",
      "Epoch 20/30\n",
      "6/6 - 0s - 30ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.0000e+00 - val_loss: 3.7452\n",
      "Epoch 21/30\n",
      "6/6 - 0s - 34ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.0000e+00 - val_loss: 3.7786\n",
      "Epoch 22/30\n",
      "6/6 - 0s - 30ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.0000e+00 - val_loss: 3.8252\n",
      "Epoch 23/30\n",
      "6/6 - 0s - 25ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.0000e+00 - val_loss: 3.8764\n",
      "Epoch 24/30\n",
      "6/6 - 0s - 28ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.0000e+00 - val_loss: 3.9278\n",
      "Epoch 25/30\n",
      "6/6 - 0s - 31ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.0000e+00 - val_loss: 3.9756\n",
      "Epoch 26/30\n",
      "6/6 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.0000e+00 - val_loss: 4.0261\n",
      "Epoch 27/30\n",
      "6/6 - 0s - 41ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.0000e+00 - val_loss: 4.0752\n",
      "Epoch 28/30\n",
      "6/6 - 0s - 65ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.0000e+00 - val_loss: 4.1224\n",
      "Epoch 29/30\n",
      "6/6 - 0s - 37ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.0000e+00 - val_loss: 4.1677\n",
      "Epoch 30/30\n",
      "6/6 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 4.2100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Train the Model\n",
    "# ------------------------------------------------------\n",
    "epochs = 30\n",
    "history = model.fit(\n",
    "    train_padded, \n",
    "    train_labels,\n",
    "    validation_data=(val_padded, val_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=2,  # small batch_size for demonstration\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 1.5233\n",
      "Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------\n",
    "# 6) Evaluate on Test Data\n",
    "# ------------------------------------------------------\n",
    "loss, accuracy = model.evaluate(test_padded, test_labels, verbose=0)\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n",
      "\n",
      "Predictions on new sentences:\n",
      "Text: i absolutely loved the film so amazing\n",
      "Predicted sentiment = Negative (score: 0.0961)\n",
      "\n",
      "Text: it was a terrible movie i regret watching\n",
      "Predicted sentiment = Negative (score: 0.0042)\n",
      "\n",
      "Text: wonderful storytelling but the acting was dull\n",
      "Predicted sentiment = Positive (score: 0.9984)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------\n",
    "# 7) Make Predictions\n",
    "# ------------------------------------------------------\n",
    "new_texts = [\n",
    "    \"i absolutely loved the film so amazing\",\n",
    "    \"it was a terrible movie i regret watching\",\n",
    "    \"wonderful storytelling but the acting was dull\"\n",
    "]\n",
    "new_padded = tokenize_and_pad(new_texts, tokenizer, max_length)\n",
    "\n",
    "predictions = model.predict(new_padded)\n",
    "print(\"\\nPredictions on new sentences:\")\n",
    "for text, pred in zip(new_texts, predictions):\n",
    "    sentiment = \"Positive\" if pred[0] > 0.5 else \"Negative\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted sentiment = {sentiment} (score: {pred[0]:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 1, 'i': 2, 'it': 3, 'this': 4, 'film': 5, 'and': 6, 'recommended': 7, 'the': 8, 'movie': 9, 'watch': 10, 'wonderful': 11, 'really': 12, 'is': 13, 'was': 14, 'of': 15, 'not': 16, 'worst': 17, 'ever': 18, 'truly': 19, 'awesome': 20, 'recommend': 21, 'highly': 22, 'so': 23, 'good': 24, 'fantastic': 25, 'will': 26, 'again': 27, 'lovely': 28, 'direction': 29, 'script': 30, 'excellent': 31, 'storyline': 32, 'liked': 33, 'love': 34, 'a': 35, 'waste': 36, 'time': 37, 'money': 38, 'dislike': 39, 'story': 40, 'dull': 41, 'horrible': 42, 'experience': 43, 'left': 44, 'midway': 45, 'hated': 46, 'every': 47, 'part': 48, 'amazing': 49, 'enjoyed': 50}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

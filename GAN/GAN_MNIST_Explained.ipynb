{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN (MNIST) — Jupyter-ready, block-by-block explanation\n",
    "\n",
    "This notebook explains each section step-by-step with detailed notes on **why** and **how** each part works. Run each cell in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Fix for OpenMP library conflict on some systems\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torchvision import datasets, transforms  # Dataset and preprocessing utilities\n",
    "from torch.utils.data import DataLoader  # For batching data\n",
    "from torchvision.utils import save_image  # Save generated images\n",
    "import matplotlib.pyplot as plt  # Visualization\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create directory to save generated images during training\n",
    "os.makedirs('generated_images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "**What is this?**\n",
    "This section imports all necessary libraries for building and training our GAN.\n",
    "\n",
    "**Why do we need these?**\n",
    "- `torch` & `torch.nn`: Core PyTorch library for building neural networks\n",
    "- `torch.optim`: Optimization algorithms (Adam optimizer)\n",
    "- `torchvision`: Provides MNIST dataset and image utilities\n",
    "- `matplotlib`: For visualizing images\n",
    "- `os`: For file and directory operations\n",
    "\n",
    "**Device Selection:** We check if CUDA (GPU) is available. Training on GPU is much faster than CPU for deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim=100, img_size=28, device=cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the GAN architecture and training\n",
    "latent_dim = 100  # Dimension of random noise vector input to generator\n",
    "img_size = 28  # Height and width of MNIST images\n",
    "channels = 1  # Number of color channels (1 for grayscale, 3 for RGB)\n",
    "batch_size = 128  # Number of images to process in parallel\n",
    "epochs = 50  # Number of complete passes through the dataset\n",
    "lr = 0.0002  # Learning rate for both networks (standard for GANs)\n",
    "beta1, beta2 = 0.5, 0.999  # Adam optimizer parameters (beta1 lower than default 0.9 for GANs)\n",
    "\n",
    "print(f'latent_dim={latent_dim}, img_size={img_size}, device={device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters Configuration\n",
    "\n",
    "**What are hyperparameters?**\n",
    "These are the settings that control how our GAN trains. They're not learned during training but set beforehand.\n",
    "\n",
    "**Key Parameters Explained:**\n",
    "- **latent_dim (100)**: Size of the random noise vector fed to the generator. Think of it as the \"seed\" that the generator uses to create images\n",
    "- **img_size (28)**: MNIST images are 28×28 pixels\n",
    "- **channels (1)**: Grayscale images have 1 channel (RGB would have 3)\n",
    "- **batch_size (128)**: Number of images processed together in one training step. Larger batches = more stable training but require more memory\n",
    "- **epochs (50)**: Number of times we'll iterate through the entire dataset\n",
    "- **lr (0.0002)**: Learning rate - controls how big the weight updates are. Too high = unstable, too low = slow training\n",
    "- **beta1, beta2 (0.5, 0.999)**: Parameters for Adam optimizer that control momentum and adaptive learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor (0-1 range)\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] range: (x - 0.5) / 0.5\n",
    "])\n",
    "\n",
    "# Download and load the MNIST training dataset\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Create DataLoader to batch and shuffle the data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get one batch to inspect\n",
    "imgs, labels = next(iter(dataloader))\n",
    "print('batch size', imgs.shape)  # Should be [128, 1, 28, 28]\n",
    "\n",
    "# Denormalize function to convert [-1, 1] back to [0, 1] for visualization\n",
    "def denorm(x): return (x + 1) / 2\n",
    "\n",
    "# Display 6 sample images from the dataset\n",
    "fig, axs = plt.subplots(1, 6, figsize=(10, 2))\n",
    "for i in range(6):\n",
    "    axs[i].imshow(denorm(imgs[i]).squeeze(), cmap='gray')  # Remove channel dimension for imshow\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare MNIST Dataset\n",
    "\n",
    "**What is MNIST?**\n",
    "MNIST is a dataset of 70,000 handwritten digits (0-9), each 28×28 pixels. It's a classic dataset for learning machine learning.\n",
    "\n",
    "**Data Preprocessing:**\n",
    "- **ToTensor()**: Converts PIL images to PyTorch tensors (multi-dimensional arrays)\n",
    "- **Normalize([0.5], [0.5])**: Scales pixel values from [0, 1] to [-1, 1]. This matches the output range of our Generator's Tanh activation\n",
    "\n",
    "**DataLoader:**\n",
    "Creates batches of images and shuffles them for better training. Shuffling prevents the model from learning based on the order of data.\n",
    "\n",
    "**Visualization:**\n",
    "We display 6 sample images to verify the data loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake.shape torch.Size([8, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network that creates fake images from random noise.\n",
    "    Takes a latent vector (random noise) and progressively transforms it into an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=100, img_size=28, channels=1):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Sequential stack of layers that transform noise into image\n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: Expand from latent_dim to 256\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU allows small negative values (better gradients)\n",
    "            \n",
    "            # Layer 2: Expand from 256 to 512\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 3: Expand from 512 to 1024\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output layer: Transform to image size (784 = 28×28×1)\n",
    "            nn.Linear(1024, img_size * img_size * channels),\n",
    "            nn.Tanh()  # Tanh outputs values in [-1, 1] to match normalized data\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass: Convert noise vector to image\n",
    "        Args:\n",
    "            z: Random noise tensor of shape [batch_size, latent_dim]\n",
    "        Returns:\n",
    "            Generated image tensor of shape [batch_size, channels, img_size, img_size]\n",
    "        \"\"\"\n",
    "        img = self.model(z)  # Pass through network\n",
    "        # Reshape flat output to image format: [batch, 1, 28, 28]\n",
    "        return img.view(z.size(0), self.channels, self.img_size, self.img_size)\n",
    "\n",
    "\n",
    "# Create generator instance and move to device (GPU/CPU)\n",
    "generator = Generator(latent_dim, img_size, channels).to(device)\n",
    "\n",
    "# Test the generator with random noise\n",
    "\n",
    " # Create 8 random noise vectors\n",
    "fake = generator(z)  # Generate 8 fake images\n",
    "print('fake.shape', fake.shape)  # Should be [8, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generator Network\n",
    "\n",
    "**What is the Generator?**\n",
    "The Generator is a neural network that creates fake images from random noise. It learns to produce realistic-looking handwritten digits.\n",
    "\n",
    "**Architecture:**\n",
    "- **Input**: Random noise vector (latent_dim = 100 dimensions)\n",
    "- **Hidden Layers**: Three fully connected layers (256 → 512 → 1024 neurons) with LeakyReLU activation\n",
    "- **Output**: 784 neurons (28×28 pixels) with Tanh activation → reshaped to [batch, 1, 28, 28]\n",
    "\n",
    "**Why this architecture?**\n",
    "- **Linear layers**: Transform and expand the noise vector into an image\n",
    "- **LeakyReLU(0.2)**: Activation function that helps with gradient flow (allows small negative values)\n",
    "- **Tanh**: Output activation that produces values in [-1, 1] range, matching our normalized data\n",
    "\n",
    "**How it works:**\n",
    "Random noise → Gradually expand dimensions → Transform into image structure → Output fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network that classifies images as real or fake.\n",
    "    Binary classifier that outputs a probability between 0 (fake) and 1 (real).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=28, channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sequential stack of layers for classification\n",
    "        self.model = nn.Sequential(\n",
    "            # Input layer: Flatten image (784) to 512 features\n",
    "            nn.Linear(img_size * img_size * channels, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Non-linear activation\n",
    "            \n",
    "            # Hidden layer: 512 to 256 features\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output layer: 256 to 1 (probability)\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Sigmoid outputs probability in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Forward pass: Classify image as real or fake\n",
    "        Args:\n",
    "            img: Image tensor of shape [batch_size, channels, img_size, img_size]\n",
    "        Returns:\n",
    "            Probability tensor of shape [batch_size, 1] (0=fake, 1=real)\n",
    "        \"\"\"\n",
    "        img_flat = img.view(img.size(0), -1)  # Flatten image: [batch, 1, 28, 28] → [batch, 784]\n",
    "        return self.model(img_flat)  # Return probability\n",
    "\n",
    "\n",
    "# Create discriminator instance and move to device\n",
    "discriminator = Discriminator(img_size, channels).to(device)\n",
    "print(discriminator)  # Display architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discriminator Network\n",
    "\n",
    "**What is the Discriminator?**\n",
    "The Discriminator is a binary classifier that determines whether an image is real (from MNIST) or fake (from Generator). It's like an art expert trying to spot forgeries.\n",
    "\n",
    "**Architecture:**\n",
    "- **Input**: Flattened image (784 pixels)\n",
    "- **Hidden Layers**: Two fully connected layers (512 → 256 neurons) with LeakyReLU\n",
    "- **Output**: Single neuron with Sigmoid activation → probability between 0 and 1\n",
    "\n",
    "**Why this architecture?**\n",
    "- **Simpler than Generator**: Classification is easier than generation\n",
    "- **Sigmoid output**: Produces probability (0 = fake, 1 = real)\n",
    "- **LeakyReLU**: Helps prevent \"dying neurons\" problem\n",
    "\n",
    "**The Adversarial Game:**\n",
    "- Generator tries to fool the Discriminator (make fake images look real)\n",
    "- Discriminator tries to correctly identify real vs fake\n",
    "- This competition drives both networks to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function: Binary Cross Entropy\n",
    "# Measures the difference between predicted and actual labels\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizer for Generator: Updates Generator weights to fool Discriminator\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Optimizer for Discriminator: Updates Discriminator weights to better classify real/fake\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights from a normal distribution.\n",
    "    Called on each layer of the networks.\n",
    "    \n",
    "    Args:\n",
    "        m: A layer module from the neural network\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):  # Only initialize Linear layers\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  # Mean=0, Std=0.02\n",
    "        if m.bias is not None: \n",
    "            nn.init.constant_(m.bias.data, 0)  # Biases initialized to 0\n",
    "\n",
    "# Apply weight initialization to both networks\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function, Optimizers, and Weight Initialization\n",
    "\n",
    "**Loss Function - Binary Cross Entropy (BCE):**\n",
    "- Measures how well the Discriminator classifies real vs fake images\n",
    "- Used for both Generator and Discriminator training\n",
    "- Formula penalizes incorrect predictions\n",
    "\n",
    "**Optimizers - Adam:**\n",
    "- Adaptive learning rate optimization algorithm\n",
    "- Separate optimizer for each network (they have opposite goals!)\n",
    "- **Beta1 = 0.5**: Lower than default (0.9) for GANs to reduce momentum and prevent oscillation\n",
    "- **Beta2 = 0.999**: Standard value for adaptive learning rate\n",
    "\n",
    "**Weight Initialization:**\n",
    "- Random initialization from normal distribution (mean=0, std=0.02)\n",
    "- Important for stable training - bad initialization can prevent learning\n",
    "- Standard practice from DCGAN paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: generated_images/epoch_0.png\n"
     ]
    }
   ],
   "source": [
    "def save_sample(generator, epoch, path='generated_images', nrow=5, nz=latent_dim):\n",
    "    \"\"\"\n",
    "    Generate and save a grid of sample images from the Generator.\n",
    "    This allows us to visually track the Generator's progress during training.\n",
    "    \n",
    "    Args:\n",
    "        generator: The Generator model\n",
    "        epoch: Current epoch number (used in filename)\n",
    "        path: Directory to save images\n",
    "        nrow: Number of images per row in the grid\n",
    "        nz: Dimension of latent noise vector\n",
    "    \"\"\"\n",
    "    generator.eval()  # Set to evaluation mode (affects dropout, batch norm, etc.)\n",
    "    os.makedirs(path, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        # Generate random noise vectors\n",
    "        z = torch.randn(nrow * nrow, nz, device=device)\n",
    "        \n",
    "        # Generate fake images from noise\n",
    "        gen_imgs = generator(z).detach().cpu()  # Move to CPU for saving\n",
    "        \n",
    "        # Save grid of images to file\n",
    "        # save_image(gen_imgs, f'{path}/epoch_{epoch}.png', nrow=nrow, \n",
    "        #            normalize=True, range=(-1,1))  # Normalize from [-1,1] to [0,1]\n",
    "        save_image(gen_imgs,\n",
    "           f'{path}/epoch_{epoch}.png',\n",
    "           nrow=nrow,\n",
    "           normalize=True)\n",
    "    \n",
    "    generator.train()  # Set back to training mode\n",
    "    print(f'Saved: {path}/epoch_{epoch}.png')\n",
    "\n",
    "# Generate and save initial (untrained) samples to see baseline\n",
    "save_sample(generator, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image Saving Function\n",
    "\n",
    "**What does this do?**\n",
    "This function generates and saves sample images during training so we can visualize the Generator's progress.\n",
    "\n",
    "**Key features:**\n",
    "- **generator.eval()**: Sets Generator to evaluation mode (disables dropout, etc.)\n",
    "- **torch.no_grad()**: Disables gradient computation for efficiency (we're not training here)\n",
    "- **save_image()**: Saves a grid of generated images to a PNG file\n",
    "- **normalize=True, range=(-1,1)**: Converts from [-1, 1] to [0, 1] for proper display\n",
    "\n",
    "**Why is this important?**\n",
    "Visual inspection is crucial for GANs. We can see if the Generator is improving, mode collapsing, or failing to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(generator, discriminator, dataloader, epoch):\n",
    "    \"\"\"\n",
    "    Train both Generator and Discriminator for one complete epoch.\n",
    "    \n",
    "    Args:\n",
    "        generator: Generator network\n",
    "        discriminator: Discriminator network\n",
    "        dataloader: DataLoader containing training images\n",
    "        epoch: Current epoch number\n",
    "    \n",
    "    Returns:\n",
    "        Final discriminator and generator loss values for the epoch\n",
    "    \"\"\"\n",
    "    for i, (imgs, _) in enumerate(dataloader):  # Iterate through batches (ignore labels)\n",
    "        imgs = imgs.to(device)  # Move images to GPU/CPU\n",
    "        \n",
    "        # Create label tensors for real and fake images\n",
    "        real = torch.ones((imgs.size(0), 1), device=device)   # Label 1 for real images\n",
    "        fake = torch.zeros((imgs.size(0), 1), device=device)  # Label 0 for fake images\n",
    "\n",
    "        # ==================== TRAIN GENERATOR ====================\n",
    "        optimizer_G.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Generate fake images from random noise\n",
    "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Calculate Generator loss: how well it fools the Discriminator\n",
    "        # We want Discriminator to output 1 (real) for generated images\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), real)\n",
    "        \n",
    "        g_loss.backward()  # Backpropagate gradients\n",
    "        optimizer_G.step()  # Update Generator weights\n",
    "\n",
    "        # ==================== TRAIN DISCRIMINATOR ====================\n",
    "        optimizer_D.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Loss on real images: should output 1\n",
    "        real_loss = adversarial_loss(discriminator(imgs), real)\n",
    "        \n",
    "        # Loss on fake images: should output 0\n",
    "        # .detach() prevents gradients from flowing back to Generator\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        \n",
    "        # Total discriminator loss: average of real and fake losses\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()  # Backpropagate gradients\n",
    "        optimizer_D.step()  # Update Discriminator weights\n",
    "\n",
    "        # Print progress every 200 batches\n",
    "        if i % 200 == 0:\n",
    "            print(f'Epoch {epoch} [Batch {i}/{len(dataloader)}] '\n",
    "                  f'D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}')\n",
    "\n",
    "    # Save sample images at the end of each epoch\n",
    "    save_sample(generator, epoch)\n",
    "    return d_loss.item(), g_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Function (One Epoch)\n",
    "\n",
    "**What happens in one training epoch?**\n",
    "An epoch is one complete pass through the entire dataset. For each batch of real images, we:\n",
    "\n",
    "**Step 1: Train Generator**\n",
    "- Create random noise → Generate fake images\n",
    "- Pass fake images through Discriminator\n",
    "- **Goal**: Maximize Discriminator's output (make it think fake images are real)\n",
    "- Calculate loss and update Generator weights\n",
    "\n",
    "**Step 2: Train Discriminator**\n",
    "- Pass real images through Discriminator → should output ~1 (real)\n",
    "- Pass fake images through Discriminator → should output ~0 (fake)\n",
    "- **Goal**: Correctly classify both real and fake images\n",
    "- Calculate combined loss and update Discriminator weights\n",
    "\n",
    "**Key Points:**\n",
    "- `.detach()`: Detaches fake images from Generator's computation graph when training Discriminator (prevents Generator from being updated)\n",
    "- Separate optimizers allow independent weight updates\n",
    "- The adversarial game: Generator wants high Discriminator output, Discriminator wants low output for fakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Batch 0/469] D_loss: 0.5032 G_loss: 0.8279\n",
      "Epoch 1 [Batch 200/469] D_loss: 0.5642 G_loss: 0.7590\n",
      "Epoch 1 [Batch 400/469] D_loss: 0.2373 G_loss: 1.3998\n",
      "Saved: generated_images/epoch_1.png\n",
      "Finished epoch 1: D_loss=0.3473 G_loss=1.1468\n",
      "Epoch 2 [Batch 0/469] D_loss: 0.3818 G_loss: 1.7525\n",
      "Epoch 2 [Batch 200/469] D_loss: 0.2885 G_loss: 1.4396\n",
      "Epoch 2 [Batch 400/469] D_loss: 0.3909 G_loss: 1.7595\n",
      "Saved: generated_images/epoch_2.png\n",
      "Finished epoch 2: D_loss=0.3617 G_loss=1.2263\n",
      "Epoch 3 [Batch 0/469] D_loss: 0.3332 G_loss: 1.3618\n",
      "Epoch 3 [Batch 200/469] D_loss: 0.2858 G_loss: 1.4420\n",
      "Epoch 3 [Batch 400/469] D_loss: 0.3559 G_loss: 2.2802\n",
      "Saved: generated_images/epoch_3.png\n",
      "Finished epoch 3: D_loss=0.2385 G_loss=2.0073\n",
      "Epoch 4 [Batch 0/469] D_loss: 0.2340 G_loss: 1.6172\n",
      "Epoch 4 [Batch 200/469] D_loss: 0.4383 G_loss: 3.1337\n",
      "Epoch 4 [Batch 400/469] D_loss: 0.2445 G_loss: 1.8604\n",
      "Saved: generated_images/epoch_4.png\n",
      "Finished epoch 4: D_loss=0.3621 G_loss=1.2126\n",
      "Epoch 5 [Batch 0/469] D_loss: 0.3457 G_loss: 1.7208\n",
      "Epoch 5 [Batch 200/469] D_loss: 0.3512 G_loss: 3.1685\n",
      "Epoch 5 [Batch 400/469] D_loss: 0.4845 G_loss: 0.7582\n",
      "Saved: generated_images/epoch_5.png\n",
      "Finished epoch 5: D_loss=0.2403 G_loss=1.9510\n",
      "Epoch 6 [Batch 0/469] D_loss: 0.3032 G_loss: 1.5011\n",
      "Epoch 6 [Batch 200/469] D_loss: 0.3725 G_loss: 1.0991\n",
      "Epoch 6 [Batch 400/469] D_loss: 0.4325 G_loss: 0.7247\n",
      "Saved: generated_images/epoch_6.png\n",
      "Finished epoch 6: D_loss=0.7697 G_loss=3.6229\n",
      "Epoch 7 [Batch 0/469] D_loss: 0.6194 G_loss: 0.4367\n",
      "Epoch 7 [Batch 200/469] D_loss: 0.3457 G_loss: 1.4767\n",
      "Epoch 7 [Batch 400/469] D_loss: 0.4043 G_loss: 1.0764\n",
      "Saved: generated_images/epoch_7.png\n",
      "Finished epoch 7: D_loss=0.3235 G_loss=1.4112\n",
      "Epoch 8 [Batch 0/469] D_loss: 0.3739 G_loss: 1.4988\n",
      "Epoch 8 [Batch 200/469] D_loss: 0.3893 G_loss: 1.0941\n",
      "Epoch 8 [Batch 400/469] D_loss: 0.3848 G_loss: 2.1106\n",
      "Saved: generated_images/epoch_8.png\n",
      "Finished epoch 8: D_loss=0.3758 G_loss=1.5246\n",
      "Epoch 9 [Batch 0/469] D_loss: 0.3467 G_loss: 1.3023\n",
      "Epoch 9 [Batch 200/469] D_loss: 0.3431 G_loss: 1.9588\n",
      "Epoch 9 [Batch 400/469] D_loss: 0.5000 G_loss: 0.6723\n",
      "Saved: generated_images/epoch_9.png\n",
      "Finished epoch 9: D_loss=0.4414 G_loss=2.7143\n",
      "Epoch 10 [Batch 0/469] D_loss: 0.3963 G_loss: 0.8497\n",
      "Epoch 10 [Batch 200/469] D_loss: 0.3093 G_loss: 2.4840\n",
      "Epoch 10 [Batch 400/469] D_loss: 0.2962 G_loss: 1.5090\n",
      "Saved: generated_images/epoch_10.png\n",
      "Finished epoch 10: D_loss=0.2937 G_loss=1.9030\n",
      "Epoch 11 [Batch 0/469] D_loss: 0.3026 G_loss: 1.4258\n",
      "Epoch 11 [Batch 200/469] D_loss: 0.3791 G_loss: 2.5645\n",
      "Epoch 11 [Batch 400/469] D_loss: 0.3825 G_loss: 0.9622\n",
      "Saved: generated_images/epoch_11.png\n",
      "Finished epoch 11: D_loss=0.7529 G_loss=3.5746\n",
      "Epoch 12 [Batch 0/469] D_loss: 0.5392 G_loss: 0.6637\n",
      "Epoch 12 [Batch 200/469] D_loss: 0.4794 G_loss: 2.5958\n",
      "Epoch 12 [Batch 400/469] D_loss: 0.3568 G_loss: 1.3713\n",
      "Saved: generated_images/epoch_12.png\n",
      "Finished epoch 12: D_loss=0.3636 G_loss=1.5762\n",
      "Epoch 13 [Batch 0/469] D_loss: 0.3595 G_loss: 1.3837\n",
      "Epoch 13 [Batch 200/469] D_loss: 0.4168 G_loss: 1.2235\n",
      "Epoch 13 [Batch 400/469] D_loss: 0.4232 G_loss: 0.9432\n",
      "Saved: generated_images/epoch_13.png\n",
      "Finished epoch 13: D_loss=0.4589 G_loss=2.6505\n",
      "Epoch 14 [Batch 0/469] D_loss: 0.3772 G_loss: 0.9907\n",
      "Epoch 14 [Batch 200/469] D_loss: 0.5120 G_loss: 0.7057\n",
      "Epoch 14 [Batch 400/469] D_loss: 0.4572 G_loss: 1.1493\n",
      "Saved: generated_images/epoch_14.png\n",
      "Finished epoch 14: D_loss=0.4523 G_loss=0.9270\n",
      "Epoch 15 [Batch 0/469] D_loss: 0.4513 G_loss: 1.9330\n",
      "Epoch 15 [Batch 200/469] D_loss: 0.5263 G_loss: 0.8198\n",
      "Epoch 15 [Batch 400/469] D_loss: 0.3821 G_loss: 1.4299\n",
      "Saved: generated_images/epoch_15.png\n",
      "Finished epoch 15: D_loss=0.6744 G_loss=2.6468\n",
      "Epoch 16 [Batch 0/469] D_loss: 0.6368 G_loss: 0.5001\n",
      "Epoch 16 [Batch 200/469] D_loss: 0.4487 G_loss: 1.5823\n",
      "Epoch 16 [Batch 400/469] D_loss: 0.5242 G_loss: 0.6538\n",
      "Saved: generated_images/epoch_16.png\n",
      "Finished epoch 16: D_loss=0.5472 G_loss=1.1815\n",
      "Epoch 17 [Batch 0/469] D_loss: 0.5073 G_loss: 0.8227\n",
      "Epoch 17 [Batch 200/469] D_loss: 0.4795 G_loss: 1.1622\n",
      "Epoch 17 [Batch 400/469] D_loss: 0.5136 G_loss: 1.3726\n",
      "Saved: generated_images/epoch_17.png\n",
      "Finished epoch 17: D_loss=0.5541 G_loss=1.1600\n",
      "Epoch 18 [Batch 0/469] D_loss: 0.4967 G_loss: 1.0803\n",
      "Epoch 18 [Batch 200/469] D_loss: 0.5888 G_loss: 1.8184\n",
      "Epoch 18 [Batch 400/469] D_loss: 0.5675 G_loss: 1.3283\n",
      "Saved: generated_images/epoch_18.png\n",
      "Finished epoch 18: D_loss=0.5684 G_loss=0.7706\n",
      "Epoch 19 [Batch 0/469] D_loss: 0.5677 G_loss: 1.4968\n",
      "Epoch 19 [Batch 200/469] D_loss: 0.6015 G_loss: 1.7329\n",
      "Epoch 19 [Batch 400/469] D_loss: 0.5339 G_loss: 1.2241\n",
      "Saved: generated_images/epoch_19.png\n",
      "Finished epoch 19: D_loss=0.4873 G_loss=0.8366\n",
      "Epoch 20 [Batch 0/469] D_loss: 0.5207 G_loss: 1.2648\n",
      "Epoch 20 [Batch 200/469] D_loss: 0.6099 G_loss: 1.6323\n",
      "Epoch 20 [Batch 400/469] D_loss: 0.6594 G_loss: 0.5545\n",
      "Saved: generated_images/epoch_20.png\n",
      "Finished epoch 20: D_loss=0.5351 G_loss=1.0023\n",
      "Epoch 21 [Batch 0/469] D_loss: 0.5388 G_loss: 1.0244\n",
      "Epoch 21 [Batch 200/469] D_loss: 0.6127 G_loss: 0.7558\n",
      "Epoch 21 [Batch 400/469] D_loss: 0.5363 G_loss: 0.9579\n",
      "Saved: generated_images/epoch_21.png\n",
      "Finished epoch 21: D_loss=0.6810 G_loss=1.6723\n",
      "Epoch 22 [Batch 0/469] D_loss: 0.5932 G_loss: 0.7772\n",
      "Epoch 22 [Batch 200/469] D_loss: 0.5506 G_loss: 0.8776\n",
      "Epoch 22 [Batch 400/469] D_loss: 0.5365 G_loss: 0.8780\n",
      "Saved: generated_images/epoch_22.png\n",
      "Finished epoch 22: D_loss=0.5576 G_loss=0.8786\n",
      "Epoch 23 [Batch 0/469] D_loss: 0.5799 G_loss: 0.9350\n",
      "Epoch 23 [Batch 200/469] D_loss: 0.6073 G_loss: 0.8227\n",
      "Epoch 23 [Batch 400/469] D_loss: 0.5458 G_loss: 1.3354\n",
      "Saved: generated_images/epoch_23.png\n",
      "Finished epoch 23: D_loss=0.5079 G_loss=1.4256\n",
      "Epoch 24 [Batch 0/469] D_loss: 0.5898 G_loss: 0.7231\n",
      "Epoch 24 [Batch 200/469] D_loss: 0.5994 G_loss: 0.7557\n",
      "Epoch 24 [Batch 400/469] D_loss: 0.5872 G_loss: 0.8308\n",
      "Saved: generated_images/epoch_24.png\n",
      "Finished epoch 24: D_loss=0.6046 G_loss=0.6254\n",
      "Epoch 25 [Batch 0/469] D_loss: 0.6141 G_loss: 1.3259\n",
      "Epoch 25 [Batch 200/469] D_loss: 0.5811 G_loss: 1.0935\n",
      "Epoch 25 [Batch 400/469] D_loss: 0.5467 G_loss: 0.9560\n",
      "Saved: generated_images/epoch_25.png\n",
      "Finished epoch 25: D_loss=0.5865 G_loss=1.3994\n",
      "Epoch 26 [Batch 0/469] D_loss: 0.6478 G_loss: 0.5639\n",
      "Epoch 26 [Batch 200/469] D_loss: 0.5545 G_loss: 1.0059\n",
      "Epoch 26 [Batch 400/469] D_loss: 0.5681 G_loss: 0.9673\n",
      "Saved: generated_images/epoch_26.png\n",
      "Finished epoch 26: D_loss=0.5366 G_loss=1.1189\n",
      "Epoch 27 [Batch 0/469] D_loss: 0.5538 G_loss: 0.9341\n",
      "Epoch 27 [Batch 200/469] D_loss: 0.6152 G_loss: 0.6941\n",
      "Epoch 27 [Batch 400/469] D_loss: 0.6111 G_loss: 0.9253\n",
      "Saved: generated_images/epoch_27.png\n",
      "Finished epoch 27: D_loss=0.6246 G_loss=0.6526\n",
      "Epoch 28 [Batch 0/469] D_loss: 0.6118 G_loss: 1.2803\n",
      "Epoch 28 [Batch 200/469] D_loss: 0.5644 G_loss: 1.2784\n",
      "Epoch 28 [Batch 400/469] D_loss: 0.5809 G_loss: 1.2943\n",
      "Saved: generated_images/epoch_28.png\n",
      "Finished epoch 28: D_loss=0.6013 G_loss=0.9176\n",
      "Epoch 29 [Batch 0/469] D_loss: 0.5271 G_loss: 0.9071\n",
      "Epoch 29 [Batch 200/469] D_loss: 0.5892 G_loss: 1.0204\n",
      "Epoch 29 [Batch 400/469] D_loss: 0.5769 G_loss: 0.8491\n",
      "Saved: generated_images/epoch_29.png\n",
      "Finished epoch 29: D_loss=0.5746 G_loss=0.8107\n",
      "Epoch 30 [Batch 0/469] D_loss: 0.5376 G_loss: 1.0668\n",
      "Epoch 30 [Batch 200/469] D_loss: 0.5332 G_loss: 0.8795\n",
      "Epoch 30 [Batch 400/469] D_loss: 0.5753 G_loss: 0.7064\n",
      "Saved: generated_images/epoch_30.png\n",
      "Finished epoch 30: D_loss=0.6859 G_loss=0.4571\n",
      "Epoch 31 [Batch 0/469] D_loss: 0.6295 G_loss: 1.4457\n",
      "Epoch 31 [Batch 200/469] D_loss: 0.5924 G_loss: 0.7807\n",
      "Epoch 31 [Batch 400/469] D_loss: 0.5665 G_loss: 0.9686\n",
      "Saved: generated_images/epoch_31.png\n",
      "Finished epoch 31: D_loss=0.5584 G_loss=1.0943\n",
      "Epoch 32 [Batch 0/469] D_loss: 0.5496 G_loss: 0.7949\n",
      "Epoch 32 [Batch 200/469] D_loss: 0.7602 G_loss: 1.9345\n",
      "Epoch 32 [Batch 400/469] D_loss: 0.5543 G_loss: 0.9501\n",
      "Saved: generated_images/epoch_32.png\n",
      "Finished epoch 32: D_loss=0.6495 G_loss=1.4925\n",
      "Epoch 33 [Batch 0/469] D_loss: 0.6478 G_loss: 0.5816\n",
      "Epoch 33 [Batch 200/469] D_loss: 0.5842 G_loss: 1.4629\n",
      "Epoch 33 [Batch 400/469] D_loss: 0.6134 G_loss: 0.7417\n",
      "Saved: generated_images/epoch_33.png\n",
      "Finished epoch 33: D_loss=0.6592 G_loss=0.5145\n",
      "Epoch 34 [Batch 0/469] D_loss: 0.6276 G_loss: 1.3694\n",
      "Epoch 34 [Batch 200/469] D_loss: 0.5837 G_loss: 1.1674\n",
      "Epoch 34 [Batch 400/469] D_loss: 0.6440 G_loss: 0.5462\n",
      "Saved: generated_images/epoch_34.png\n",
      "Finished epoch 34: D_loss=0.5733 G_loss=0.8448\n",
      "Epoch 35 [Batch 0/469] D_loss: 0.5593 G_loss: 1.0294\n",
      "Epoch 35 [Batch 200/469] D_loss: 0.6740 G_loss: 0.4804\n",
      "Epoch 35 [Batch 400/469] D_loss: 0.6174 G_loss: 0.6159\n",
      "Saved: generated_images/epoch_35.png\n",
      "Finished epoch 35: D_loss=0.5771 G_loss=0.9560\n",
      "Epoch 36 [Batch 0/469] D_loss: 0.5711 G_loss: 0.9918\n",
      "Epoch 36 [Batch 200/469] D_loss: 0.5704 G_loss: 0.7673\n",
      "Epoch 36 [Batch 400/469] D_loss: 0.5497 G_loss: 0.9673\n",
      "Saved: generated_images/epoch_36.png\n",
      "Finished epoch 36: D_loss=0.6484 G_loss=0.5667\n",
      "Epoch 37 [Batch 0/469] D_loss: 0.5805 G_loss: 1.5103\n",
      "Epoch 37 [Batch 200/469] D_loss: 0.6116 G_loss: 0.7669\n",
      "Epoch 37 [Batch 400/469] D_loss: 0.6680 G_loss: 0.5266\n",
      "Saved: generated_images/epoch_37.png\n",
      "Finished epoch 37: D_loss=0.5884 G_loss=0.8813\n",
      "Epoch 38 [Batch 0/469] D_loss: 0.5911 G_loss: 0.8929\n",
      "Epoch 38 [Batch 200/469] D_loss: 0.5642 G_loss: 1.1850\n",
      "Epoch 38 [Batch 400/469] D_loss: 0.5836 G_loss: 1.1261\n",
      "Saved: generated_images/epoch_38.png\n",
      "Finished epoch 38: D_loss=0.6027 G_loss=0.7010\n",
      "Epoch 39 [Batch 0/469] D_loss: 0.5889 G_loss: 1.3119\n",
      "Epoch 39 [Batch 200/469] D_loss: 0.5421 G_loss: 1.1499\n",
      "Epoch 39 [Batch 400/469] D_loss: 0.5630 G_loss: 0.6926\n",
      "Saved: generated_images/epoch_39.png\n",
      "Finished epoch 39: D_loss=0.5833 G_loss=1.0438\n",
      "Epoch 40 [Batch 0/469] D_loss: 0.5818 G_loss: 0.7970\n",
      "Epoch 40 [Batch 200/469] D_loss: 0.6325 G_loss: 0.5919\n",
      "Epoch 40 [Batch 400/469] D_loss: 0.5779 G_loss: 0.7552\n",
      "Saved: generated_images/epoch_40.png\n",
      "Finished epoch 40: D_loss=0.5941 G_loss=0.9054\n",
      "Epoch 41 [Batch 0/469] D_loss: 0.5695 G_loss: 0.9290\n",
      "Epoch 41 [Batch 200/469] D_loss: 0.5945 G_loss: 0.9336\n",
      "Epoch 41 [Batch 400/469] D_loss: 0.5919 G_loss: 0.9805\n",
      "Saved: generated_images/epoch_41.png\n",
      "Finished epoch 41: D_loss=0.5827 G_loss=1.1261\n",
      "Epoch 42 [Batch 0/469] D_loss: 0.5435 G_loss: 0.8587\n",
      "Epoch 42 [Batch 200/469] D_loss: 0.5712 G_loss: 1.1030\n",
      "Epoch 42 [Batch 400/469] D_loss: 0.5868 G_loss: 1.1878\n",
      "Saved: generated_images/epoch_42.png\n",
      "Finished epoch 42: D_loss=0.5682 G_loss=0.9412\n",
      "Epoch 43 [Batch 0/469] D_loss: 0.5818 G_loss: 0.9231\n",
      "Epoch 43 [Batch 200/469] D_loss: 0.5771 G_loss: 1.1756\n",
      "Epoch 43 [Batch 400/469] D_loss: 0.5703 G_loss: 0.8900\n",
      "Saved: generated_images/epoch_43.png\n",
      "Finished epoch 43: D_loss=0.6399 G_loss=1.4562\n",
      "Epoch 44 [Batch 0/469] D_loss: 0.7239 G_loss: 0.4337\n",
      "Epoch 44 [Batch 200/469] D_loss: 0.6571 G_loss: 0.5442\n",
      "Epoch 44 [Batch 400/469] D_loss: 0.6577 G_loss: 0.5320\n",
      "Saved: generated_images/epoch_44.png\n",
      "Finished epoch 44: D_loss=0.5711 G_loss=0.8781\n",
      "Epoch 45 [Batch 0/469] D_loss: 0.5429 G_loss: 0.9222\n",
      "Epoch 45 [Batch 200/469] D_loss: 0.6799 G_loss: 0.5377\n",
      "Epoch 45 [Batch 400/469] D_loss: 0.5888 G_loss: 1.2047\n",
      "Saved: generated_images/epoch_45.png\n",
      "Finished epoch 45: D_loss=0.6021 G_loss=0.9005\n",
      "Epoch 46 [Batch 0/469] D_loss: 0.6108 G_loss: 0.7975\n",
      "Epoch 46 [Batch 200/469] D_loss: 0.6526 G_loss: 0.5597\n",
      "Epoch 46 [Batch 400/469] D_loss: 0.6448 G_loss: 0.6082\n",
      "Saved: generated_images/epoch_46.png\n",
      "Finished epoch 46: D_loss=0.5812 G_loss=0.9871\n",
      "Epoch 47 [Batch 0/469] D_loss: 0.5856 G_loss: 0.8954\n",
      "Epoch 47 [Batch 200/469] D_loss: 0.6879 G_loss: 0.4700\n",
      "Epoch 47 [Batch 400/469] D_loss: 0.5706 G_loss: 0.9150\n",
      "Saved: generated_images/epoch_47.png\n",
      "Finished epoch 47: D_loss=0.5838 G_loss=0.8572\n",
      "Epoch 48 [Batch 0/469] D_loss: 0.6051 G_loss: 0.9856\n",
      "Epoch 48 [Batch 200/469] D_loss: 0.5976 G_loss: 0.8126\n",
      "Epoch 48 [Batch 400/469] D_loss: 0.6125 G_loss: 0.8332\n",
      "Saved: generated_images/epoch_48.png\n",
      "Finished epoch 48: D_loss=0.5938 G_loss=0.8984\n",
      "Epoch 49 [Batch 0/469] D_loss: 0.5846 G_loss: 0.8911\n",
      "Epoch 49 [Batch 200/469] D_loss: 0.6046 G_loss: 1.3702\n",
      "Epoch 49 [Batch 400/469] D_loss: 0.5926 G_loss: 0.9289\n",
      "Saved: generated_images/epoch_49.png\n",
      "Finished epoch 49: D_loss=0.6066 G_loss=0.6412\n",
      "Epoch 50 [Batch 0/469] D_loss: 0.5741 G_loss: 1.0422\n",
      "Epoch 50 [Batch 200/469] D_loss: 0.5615 G_loss: 0.9295\n",
      "Epoch 50 [Batch 400/469] D_loss: 0.5934 G_loss: 0.6580\n",
      "Saved: generated_images/epoch_50.png\n",
      "Finished epoch 50: D_loss=0.6310 G_loss=1.1784\n",
      "Saved model weights: generator_final.pth, discriminator_final.pth\n"
     ]
    }
   ],
   "source": [
    "# Main training loop: Train for specified number of epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train both networks for one complete pass through the dataset\n",
    "    d_loss_val, g_loss_val = train_one_epoch(generator, discriminator, dataloader, epoch)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'Finished epoch {epoch}: D_loss={d_loss_val:.4f} G_loss={g_loss_val:.4f}')\n",
    "\n",
    "# Save the trained model weights to disk\n",
    "torch.save(generator.state_dict(), 'generator_final.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_final.pth')\n",
    "print('Saved model weights: generator_final.pth, discriminator_final.pth')\n",
    "\n",
    "# To load models later, use:\n",
    "# generator.load_state_dict(torch.load('generator_final.pth'))\n",
    "# discriminator.load_state_dict(torch.load('discriminator_final.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "**What did we accomplish?**\n",
    "✅ Built a complete GAN from scratch with Generator and Discriminator networks\n",
    "✅ Trained on MNIST dataset to generate handwritten digits\n",
    "✅ Saved generated images to visualize training progress\n",
    "✅ Saved trained models for future use\n",
    "\n",
    "**Key Concepts Learned:**\n",
    "- **Adversarial Training**: Two networks competing against each other\n",
    "- **Generator**: Creates fake data from random noise\n",
    "- **Discriminator**: Distinguishes real from fake data\n",
    "- **The GAN Game**: Generator improves by fooling Discriminator, Discriminator improves by detecting fakes\n",
    "\n",
    "**What can you do next?**\n",
    "1. **Examine generated images** in the `generated_images/` folder to see the progression\n",
    "2. **Experiment with hyperparameters**: Try different learning rates, architectures, or latent dimensions\n",
    "3. **Generate new images**: Load the saved model and create new digits\n",
    "4. **Try different datasets**: Apply this same architecture to Fashion-MNIST or other datasets\n",
    "5. **Explore advanced GANs**: DCGAN (convolutional), StyleGAN, CycleGAN, etc.\n",
    "\n",
    "**Common Issues & Tips:**\n",
    "- **Mode collapse**: Generator produces same images → lower learning rate or try different architecture\n",
    "- **Training instability**: Loss oscillates wildly → adjust learning rates or use gradient penalties\n",
    "- **Poor quality**: Images stay blurry → train longer or use deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop\n",
    "\n",
    "**What happens here?**\n",
    "This is where the actual training occurs. We loop through all epochs and train both networks.\n",
    "\n",
    "**Training Process:**\n",
    "1. For each epoch (1 to 50):\n",
    "   - Call `train_one_epoch()` which processes all batches\n",
    "   - Both Generator and Discriminator are updated multiple times\n",
    "   - Sample images are saved to track progress\n",
    "2. After all epochs:\n",
    "   - Save trained model weights to disk\n",
    "   - These can be loaded later to generate new images without retraining\n",
    "\n",
    "**What to expect:**\n",
    "- **Early epochs**: Generated images look like random noise\n",
    "- **Middle epochs**: Recognizable digit shapes start to appear\n",
    "- **Later epochs**: Clear, realistic-looking handwritten digits\n",
    "- **Loss values**: D_loss should stay around 0.5-0.7 (not too high/low), G_loss may fluctuate\n",
    "\n",
    "**Time estimate:** On GPU: ~5-10 minutes, On CPU: ~30-60 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

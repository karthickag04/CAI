{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN (MNIST) — Jupyter-ready, block-by-block explanation\n",
    "\n",
    "This notebook explains each section step-by-step with detailed notes on **why** and **how** each part works. Run each cell in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\laptop\\.conda\\envs\\face\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Fix for OpenMP library conflict on some systems\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torchvision import datasets, transforms  # Dataset and preprocessing utilities\n",
    "from torch.utils.data import DataLoader  # For batching data\n",
    "from torchvision.utils import save_image  # Save generated images\n",
    "import matplotlib.pyplot as plt  # Visualization\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create directory to save generated images during training\n",
    "os.makedirs('generated_images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "**What is this?**\n",
    "This section imports all necessary libraries for building and training our GAN.\n",
    "\n",
    "**Why do we need these?**\n",
    "- `torch` & `torch.nn`: Core PyTorch library for building neural networks\n",
    "- `torch.optim`: Optimization algorithms (Adam optimizer)\n",
    "- `torchvision`: Provides MNIST dataset and image utilities\n",
    "- `matplotlib`: For visualizing images\n",
    "- `os`: For file and directory operations\n",
    "\n",
    "**Device Selection:** We check if CUDA (GPU) is available. Training on GPU is much faster than CPU for deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_dim=100, img_size=28, device=cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the GAN architecture and training\n",
    "latent_dim = 100  # Dimension of random noise vector input to generator\n",
    "img_size = 28  # Height and width of MNIST images\n",
    "channels = 1  # Number of color channels (1 for grayscale, 3 for RGB)\n",
    "batch_size = 128  # Number of images to process in parallel\n",
    "epochs = 50  # Number of complete passes through the dataset\n",
    "lr = 0.0002  # Learning rate for both networks (standard for GANs)\n",
    "beta1, beta2 = 0.5, 0.999  # Adam optimizer parameters (beta1 lower than default 0.9 for GANs)\n",
    "\n",
    "print(f'latent_dim={latent_dim}, img_size={img_size}, device={device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters Configuration\n",
    "\n",
    "**What are hyperparameters?**\n",
    "These are the settings that control how our GAN trains. They're not learned during training but set beforehand.\n",
    "\n",
    "**Key Parameters Explained:**\n",
    "- **latent_dim (100)**: Size of the random noise vector fed to the generator. Think of it as the \"seed\" that the generator uses to create images\n",
    "- **img_size (28)**: MNIST images are 28×28 pixels\n",
    "- **channels (1)**: Grayscale images have 1 channel (RGB would have 3)\n",
    "- **batch_size (128)**: Number of images processed together in one training step. Larger batches = more stable training but require more memory\n",
    "- **epochs (50)**: Number of times we'll iterate through the entire dataset\n",
    "- **lr (0.0002)**: Learning rate - controls how big the weight updates are. Too high = unstable, too low = slow training\n",
    "- **beta1, beta2 (0.5, 0.999)**: Parameters for Adam optimizer that control momentum and adaptive learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACCCAYAAAAuX9XfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEFlJREFUeJzt3XuMVOX5B/BZvKxGW4uAhJYqVvBSBG2txoJtaiOKIV7xEmuFKCgxsVUqxpRGqRBskRQvaGJDERQJmuAleMMLVWkkVmm1NBhAU9QUS7wi2KqLhYY0v/3tc8q+7MC+Mzszn89f55tzduaVOTkzj+d9ztu0devWrSUAAIBO1q2zXxAAAGAbxQYAAJCFYgMAAMhCsQEAAGSh2AAAALJQbAAAAFkoNgAAgCwUGwAAQBa7d/TApqamPCMgu1zrNjonapdzgiLnBJU4J5wPtcs1gp09J9zZAAAAslBsAAAAWSg2AACA6vZsdHXjx48PecaMGSGfdtppIT/66KMVGRcAADQqdzYAAIAsFBsAAEAWig0AACCLuunZmDBhQshbtmwJecCAARUeEQAANDZ3NgAAgCwUGwAAQBaKDQAAIIua7dno2bNnyM3NzVUbC1CfTj755Hb7wtruAzj11FNDfvzxx5PHDx8+POQnn3wyy7jYNX379g156tSpIY8ePbrCI6o97mwAAABZKDYAAIAsFBsAAEAWNduzcfzxx4fcvXv3kFetWhXy5s2bKzIuoH7MnTu3dbt3795VHQu17/LLLw9506ZNId97770VHhGd6YQTTgh569atIa9bty7ktWvXVmRc7NrnOGfOnJCL3wU33XRTyCtXrsw4utrkzgYAAJCFYgMAAMhCsQEAAGRRsz0bO1J8vvWjjz5atbEAteHLX/5yyLvvXreXSDLo06dPyIsXLw75yCOPDHnUqFEVGRf5HHLIIa3bF110UfLYsWPHhrxmzZps46Ljmpqakuul9O/fP9mLs99++3X4vbp1i/+Pv7h+05lnnhny2WefHfL69etLtcidDQAAIAvFBgAAkIViAwAAyKJmJiTvtttuIV9yySXJ45966qmQ33zzzSzjAurHyJEjQ+7Ro0e7xw4dOjTkF154Idu4qA133nlnyIMGDaraWKiM+fPnt2737ds3eWxLS0sFRkS5rrjiipAnTpyY7NGYNGlSyMuWLevwe1144YUhT5s2LXn8c889F/INN9wQ8oIFC0q1wJ0NAAAgC8UGAACQhWIDAABo7J6N/fffP+QzzjijamOhMk4//fSQH3744ZDfe++9kB955JHk6y1fvjzkv/71r+0+a7s4R7PopZdeCnnz5s3J46kNBx54YIeP1aPRmNrOy3/ooYfCvsGDB3d4fv82Cxcu7OTRkVu/fv1C/trXvtbha0Q5c/vJp7jWxWmnnZY8/tNPPw15ypQpO/3eb7/9dsjFa8ixxx4b8mGHHRby3LlzQ25ubk7u7yrc2QAAALJQbAAAAFkoNgAAgMbu2diRL774IuR//etfVRsLnWPx4sUhX3311clnYY8ePXqX1mYpx5/+9KeQp06dmhz7Z5991mnvTT7FZ6BD0QUXXNC6fcwxxySP/eijj0K+9dZbQ/788887eXTkdumll7bbs7Fu3bqwb/z48SH7vLuGE044IeRhw4aFvGbNmpBHjBjRae/9/PPPJ/PAgQNDnj59esinnnpqyL/97W9D3rJlS8jz5s0LeUf9qLm4swEAAGSh2AAAALJQbAAAAFnUTc/Gxo0bQ167dm3VxkLnaGlpCfnmm29O5hNPPDHkM888s6w1FN59993W7X/84x/JebqDBg0K+cEHH0weP3v27OR70/Xddddd1R4CVXDllVeGPHny5A7/7b777hvyyJEjQ37//fdDfvPNN3dqjFRuvacJEya0e+yCBQuSazvRNdxzzz3J/c8991zIb7zxRqlSVq5cGfJVV12VXF9s1KhRId99993t/q7ZXj9ppbizAQAAZKHYAAAAslBsAAAAWTRt7eBDd5uamkrV1KtXr5DXr1+fPH748OEhP/3006VGleu5ytU+J3Lq1i3W4fvss09ynuTMmTOTc0Ivu+yyZD9KpTkntq/4fPVDDjmkdfuxxx5LzuWudc6J/103YXvPwf/GN77Rae910UUXhTx//vxSvZ8TtXY+vPjiiyEfd9xx7f4W+f73v1+1uf6VUKvXiKFDh4a8ZMmS5PonQ4YMSfZRVNP+++8f8gMPPBDyD37wg5BXrFiRXKfjnXfeqcg54c4GAACQhWIDAADIQrEBAABkUTfrbEBn2rJlS8ibNm0qa82OYk/Hr371q5BXr169y2OkskaMGFHtIVCBHo3imjmpHo1Vq1aFfPjhh3fy6Ki04joaRx99dPL4iy++uHX7gAMOCPuGDRsW8kknnRTyrFmzQv7zn/+cXCOBjunRo0fId955Z8jNzc0hv/766122R6Poww8/DPmUU05J9mgMHjw45HHjxoU8adKkUiW4swEAAGSh2AAAALJQbAAAAFno2dgJffv2LWtNj7feeqsi46LreOGFF0LWo1F/rrnmmpCnT59etbGw8+bNmxfysccemzx+wYIF7c7vLz7z/vjjjy9rLHvttVdZ6wPQ+caPHx/ynnvumTz+17/+det2//79k+szFZ111lnJHqDf/OY3Ic+ePTv5evxXz549Qz7yyCOTx9fyOmwthTW77r777pBvvPHGXbomdRZ3NgAAgCwUGwAAQBaKDQAAIAs9G9vR1NQU8qGHHhryokWLQi7O0/zggw9Cnjx5csi33357J42Uahk5cmRy/+eff16xsdB5iuuh3HXXXe0eu6O53HRNCxcuLGsOc9sejW0uv/zy1u2NGzeGfcW8I9dff33IV111VchHHXVUyM8//3xyHQcqr+1ntGHDhrDv3nvvDXnOnDnJNV5+9rOfhXzdddclf3u89957OznqxlZcR6vYa1VP32E/+tGPkte74jUkV/+KOxsAAEAWig0AACAL06i246CDDgr5tddeK+vve/ToEfKJJ54YsmlUtadPnz4h9+rVq8OPQ6R+brd3dB9dR9tpT9uMGDEi5Obm5pA//PDDkKdMmRJyv379WrcHDhxY1mNziwYMGFDW8ccdd1xZx1NZL774YsijRo3apb//4x//GPKPf/zjkFeuXBnyU089Vdb71au99967rEfTL1++vFSvFhWm3k2cODH5ezcXdzYAAIAsFBsAAEAWig0AAKCxezY+++yzkN96663kvLNvf/vbHX6cV3E+/pNPPpl8FO6SJUuSrz116tSQTz755JC7d+8e8kcffdTu2OgainOlv/SlL4X88ssvh/zMM89UZFxANG7cuJBvvvnmsh5Z/O9//zvkW265pd1rwVe+8pVSZ9q8eXPIf/nLX0J+5JFHOvX9+N8eyz322CN5/Mcff9xuz0/xd0fx3Cs+yvaTTz4J+Y033gj5lFNOCXns2LHJ/iM9G/81ZsyY5P7i93U9P6p+1qxZyZ6N4uO1c3FnAwAAyEKxAQAAZKHYAAAAGrtnozinrvg86mLPxk9/+tPkXNe2a2cUl2/v379/8lnWZ511VnK595aWlpA3bNiQnBNM13fooYcm969YsaJiYwH+3+mnnx7ybbfdVtYc/KLiGjrFnru2PXxbt24t7YoZM2Yk59ybg5/feeedF3LPnj2Tx19yySUh//Of/2zdvvbaa8O+4cOHh3zfffcl19EoKq7/cOWVV4Z82GGHJf++UV1wwQXJ/Y30fT2wsBZQtbizAQAAZKHYAAAAslBsAAAAjd2zUeyDKD47+Jxzzilr7Yzrr7++dfuHP/xhWfP1n3jiiZC/853vtPvc7W1eeumlkDdu3Jh8P6qvuI7GFVdckTx+7ty5mUcEbM/Pf/7zXerR2JG2c/K3+fvf/97usX379g15n332Sb72K6+8ErIeja6v2MO5Zs2abJ9fcU2Y4vcS29f2M9nmu9/9bqlR9O7du93futvrf545c2ZFxuXOBgAAkIViAwAAyEKxAQAANHbPRtGzzz6bnJc2derUkL/61a+GPHv27A4/K704B3jIkCFljbXY40HXt99++4X89a9/PeQvvvgi5M2bN1dkXHQd5V4HyGPfffcNecuWLSF369Yt2XPx6quvhnz//feH/M477yS/e9pavHhxco2OHa3pceCBB4b89ttvJ/+e+nb11Vcn15RZvXp1hUdUG6ZMmRLy448/HvLEiRNDfuaZZ0Jet25dqatqLvQEn3/++SHffvvtyT6fYl9Rsb8lF3c2AACALBQbAABAFooNAAAgi5rt2Si66aabQv7Wt76VXIejkorzdn/3u99VbSx0zLXXXpvc/4tf/CK5lgr1b9myZdUeAqVSadCgQSFfeOGFybUKFi5cGPKmTZtK1TJjxoyQx40bF/IRRxxR4RE1nqVLl4b88ccfJ/v3zj333JCnTZvWbi/fjvTr1y/5vTNs2LBkP9LkyZPLer9G8be//a2stdNefvnlkJcvXx7yokWLsv2G23vvvUMePXp0yN/85jdDHj58eMgDBgxIvn7x+nbjjTeWqsGdDQAAIAvFBgAAkIViAwAAyKJp644Wmfi/A5uaSrWkR48eyXm8I0eObN3u2bNn2Hf44Yfv0nu/8soryXmYS5YsKVVSBz/istXaOVHO/NninM3iWisHHXRQyBs2bCjVEufE9l188cUdnpv70EMPhdzS0hLyAw88kMxdjXNi15W7zsYHH3wQ8ve+972QV61aVaq3c6Krnw/FNRb69OmTPP66665r3Z45c2bYd/DBByevL717906umVBUXFej0j09tXKNKL7ehAkTkn0Lu+9eXvtyOb05K1asSO4/6qijQt5tt93KGsu7776b7B+94YYbQi7+tqnUOeHOBgAAkIViAwAAyEKxAQAAZFG3PRvU3jzLahs6dGjr9h/+8Iewb/r06WWtw9HVOSc69szztWvXtm736tUr+d9a/DcdM2ZMyHPnzi11Zc6Jyvds3HHHHSH/5Cc/yTKundWIPRvFPq1in0XO8Rf/vefNm5dcT+y1114rVVK9XCOK65f88pe/DHnIkCGlallR6PH4/e9/n+z5LfZgrF+/vlRJejYAAICqUmwAAABZKDYAAIAsynu4MNSxOXPmtG5/8sknybnV1KdPP/005FtvvbV1e9KkScl5xsV5v/Pnz88yRmpXcX7zsmXLqjYWtm/s2LEhv/rqqyEXrwPFNb12ZU2PyZMnhzxr1qydfm3a9/TTT4e8dOnS5DpaxxxzTFk9HW3PoTVr1oR9gwcPDvnZZ58N+Ywzzgh506ZNpXrgzgYAAJCFYgMAAMhCsQEAAGRhnY0GUC/Pxu5sxbmTbefmvv/++2HfAQccUKonzgmKnBO77pprrgl52rRpNbWuRlEjrrNB+1wjKLLOBgAAUFWKDQAAIAvFBgAAkIWejQZgnuX2jRkzpt1nmuvZaMxzopE5JyjSs0FbrhEU6dkAAACqSrEBAABkodgAAACy2D3Py0LtaTtvdPXq1VUdCwBAPXBnAwAAyEKxAQAAZKHYAAAAsrDORgPwbOzt6927d8hLly5t3e7evXvYZ52NxjgnGplzgiLrbNCWawRF1tkAAACqSrEBAABkodgAAACy0LPRAMyzpMg5QZFzgiI9G7TlGkGRng0AAKCqFBsAAEAWig0AAKC6PRsAAADlcGcDAADIQrEBAABkodgAAACyUGwAAABZKDYAAIAsFBsAAEAWig0AACALxQYAAJCFYgMAACjl8B/1LsQoqgsAdQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor (0-1 range)\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] range: (x - 0.5) / 0.5\n",
    "])\n",
    "\n",
    "# Download and load the MNIST training dataset\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Create DataLoader to batch and shuffle the data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get one batch to inspect\n",
    "imgs, labels = next(iter(dataloader))\n",
    "print('batch size', imgs.shape)  # Should be [128, 1, 28, 28]\n",
    "\n",
    "# Denormalize function to convert [-1, 1] back to [0, 1] for visualization\n",
    "def denorm(x): return (x + 1) / 2\n",
    "\n",
    "# Display 6 sample images from the dataset\n",
    "fig, axs = plt.subplots(1, 6, figsize=(10, 2))\n",
    "for i in range(6):\n",
    "    axs[i].imshow(denorm(imgs[i]).squeeze(), cmap='gray')  # Remove channel dimension for imshow\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare MNIST Dataset\n",
    "\n",
    "**What is MNIST?**\n",
    "MNIST is a dataset of 70,000 handwritten digits (0-9), each 28×28 pixels. It's a classic dataset for learning machine learning.\n",
    "\n",
    "**Data Preprocessing:**\n",
    "- **ToTensor()**: Converts PIL images to PyTorch tensors (multi-dimensional arrays)\n",
    "- **Normalize([0.5], [0.5])**: Scales pixel values from [0, 1] to [-1, 1]. This matches the output range of our Generator's Tanh activation\n",
    "\n",
    "**DataLoader:**\n",
    "Creates batches of images and shuffles them for better training. Shuffling prevents the model from learning based on the order of data.\n",
    "\n",
    "**Visualization:**\n",
    "We display 6 sample images to verify the data loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network that creates fake images from random noise.\n",
    "    Takes a latent vector (random noise) and progressively transforms it into an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=100, img_size=28, channels=1):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Sequential stack of layers that transform noise into image\n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: Expand from latent_dim to 256\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU allows small negative values (better gradients)\n",
    "            \n",
    "            # Layer 2: Expand from 256 to 512\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 3: Expand from 512 to 1024\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output layer: Transform to image size (784 = 28×28×1)\n",
    "            nn.Linear(1024, img_size * img_size * channels),\n",
    "            nn.Tanh()  # Tanh outputs values in [-1, 1] to match normalized data\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass: Convert noise vector to image\n",
    "        Args:\n",
    "            z: Random noise tensor of shape [batch_size, latent_dim]\n",
    "        Returns:\n",
    "            Generated image tensor of shape [batch_size, channels, img_size, img_size]\n",
    "        \"\"\"\n",
    "        img = self.model(z)  # Pass through network\n",
    "        # Reshape flat output to image format: [batch, 1, 28, 28]\n",
    "        return img.view(z.size(0), self.channels, self.img_size, self.img_size)\n",
    "\n",
    "\n",
    "# Create generator instance and move to device (GPU/CPU)\n",
    "generator = Generator(latent_dim, img_size, channels).to(device)\n",
    "\n",
    "# Test the generator with random noise\n",
    "z = torch.randn(8, latent_dim, device=device)  # Create 8 random noise vectors\n",
    "fake = generator(z)  # Generate 8 fake images\n",
    "print('fake.shape', fake.shape)  # Should be [8, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generator Network\n",
    "\n",
    "**What is the Generator?**\n",
    "The Generator is a neural network that creates fake images from random noise. It learns to produce realistic-looking handwritten digits.\n",
    "\n",
    "**Architecture:**\n",
    "- **Input**: Random noise vector (latent_dim = 100 dimensions)\n",
    "- **Hidden Layers**: Three fully connected layers (256 → 512 → 1024 neurons) with LeakyReLU activation\n",
    "- **Output**: 784 neurons (28×28 pixels) with Tanh activation → reshaped to [batch, 1, 28, 28]\n",
    "\n",
    "**Why this architecture?**\n",
    "- **Linear layers**: Transform and expand the noise vector into an image\n",
    "- **LeakyReLU(0.2)**: Activation function that helps with gradient flow (allows small negative values)\n",
    "- **Tanh**: Output activation that produces values in [-1, 1] range, matching our normalized data\n",
    "\n",
    "**How it works:**\n",
    "Random noise → Gradually expand dimensions → Transform into image structure → Output fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network that classifies images as real or fake.\n",
    "    Binary classifier that outputs a probability between 0 (fake) and 1 (real).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=28, channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sequential stack of layers for classification\n",
    "        self.model = nn.Sequential(\n",
    "            # Input layer: Flatten image (784) to 512 features\n",
    "            nn.Linear(img_size * img_size * channels, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # Non-linear activation\n",
    "            \n",
    "            # Hidden layer: 512 to 256 features\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output layer: 256 to 1 (probability)\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Sigmoid outputs probability in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Forward pass: Classify image as real or fake\n",
    "        Args:\n",
    "            img: Image tensor of shape [batch_size, channels, img_size, img_size]\n",
    "        Returns:\n",
    "            Probability tensor of shape [batch_size, 1] (0=fake, 1=real)\n",
    "        \"\"\"\n",
    "        img_flat = img.view(img.size(0), -1)  # Flatten image: [batch, 1, 28, 28] → [batch, 784]\n",
    "        return self.model(img_flat)  # Return probability\n",
    "\n",
    "\n",
    "# Create discriminator instance and move to device\n",
    "discriminator = Discriminator(img_size, channels).to(device)\n",
    "print(discriminator)  # Display architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discriminator Network\n",
    "\n",
    "**What is the Discriminator?**\n",
    "The Discriminator is a binary classifier that determines whether an image is real (from MNIST) or fake (from Generator). It's like an art expert trying to spot forgeries.\n",
    "\n",
    "**Architecture:**\n",
    "- **Input**: Flattened image (784 pixels)\n",
    "- **Hidden Layers**: Two fully connected layers (512 → 256 neurons) with LeakyReLU\n",
    "- **Output**: Single neuron with Sigmoid activation → probability between 0 and 1\n",
    "\n",
    "**Why this architecture?**\n",
    "- **Simpler than Generator**: Classification is easier than generation\n",
    "- **Sigmoid output**: Produces probability (0 = fake, 1 = real)\n",
    "- **LeakyReLU**: Helps prevent \"dying neurons\" problem\n",
    "\n",
    "**The Adversarial Game:**\n",
    "- Generator tries to fool the Discriminator (make fake images look real)\n",
    "- Discriminator tries to correctly identify real vs fake\n",
    "- This competition drives both networks to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Binary Cross Entropy\n",
    "# Measures the difference between predicted and actual labels\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizer for Generator: Updates Generator weights to fool Discriminator\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Optimizer for Discriminator: Updates Discriminator weights to better classify real/fake\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights from a normal distribution.\n",
    "    Called on each layer of the networks.\n",
    "    \n",
    "    Args:\n",
    "        m: A layer module from the neural network\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):  # Only initialize Linear layers\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  # Mean=0, Std=0.02\n",
    "        if m.bias is not None: \n",
    "            nn.init.constant_(m.bias.data, 0)  # Biases initialized to 0\n",
    "\n",
    "# Apply weight initialization to both networks\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function, Optimizers, and Weight Initialization\n",
    "\n",
    "**Loss Function - Binary Cross Entropy (BCE):**\n",
    "- Measures how well the Discriminator classifies real vs fake images\n",
    "- Used for both Generator and Discriminator training\n",
    "- Formula penalizes incorrect predictions\n",
    "\n",
    "**Optimizers - Adam:**\n",
    "- Adaptive learning rate optimization algorithm\n",
    "- Separate optimizer for each network (they have opposite goals!)\n",
    "- **Beta1 = 0.5**: Lower than default (0.9) for GANs to reduce momentum and prevent oscillation\n",
    "- **Beta2 = 0.999**: Standard value for adaptive learning rate\n",
    "\n",
    "**Weight Initialization:**\n",
    "- Random initialization from normal distribution (mean=0, std=0.02)\n",
    "- Important for stable training - bad initialization can prevent learning\n",
    "- Standard practice from DCGAN paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(generator, epoch, path='generated_images', nrow=5, nz=latent_dim):\n",
    "    \"\"\"\n",
    "    Generate and save a grid of sample images from the Generator.\n",
    "    This allows us to visually track the Generator's progress during training.\n",
    "    \n",
    "    Args:\n",
    "        generator: The Generator model\n",
    "        epoch: Current epoch number (used in filename)\n",
    "        path: Directory to save images\n",
    "        nrow: Number of images per row in the grid\n",
    "        nz: Dimension of latent noise vector\n",
    "    \"\"\"\n",
    "    generator.eval()  # Set to evaluation mode (affects dropout, batch norm, etc.)\n",
    "    os.makedirs(path, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        # Generate random noise vectors\n",
    "        z = torch.randn(nrow * nrow, nz, device=device)\n",
    "        \n",
    "        # Generate fake images from noise\n",
    "        gen_imgs = generator(z).detach().cpu()  # Move to CPU for saving\n",
    "        \n",
    "        # Save grid of images to file\n",
    "        save_image(gen_imgs, f'{path}/epoch_{epoch}.png', nrow=nrow, \n",
    "                   normalize=True, range=(-1,1))  # Normalize from [-1,1] to [0,1]\n",
    "    \n",
    "    generator.train()  # Set back to training mode\n",
    "    print(f'Saved: {path}/epoch_{epoch}.png')\n",
    "\n",
    "# Generate and save initial (untrained) samples to see baseline\n",
    "save_sample(generator, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image Saving Function\n",
    "\n",
    "**What does this do?**\n",
    "This function generates and saves sample images during training so we can visualize the Generator's progress.\n",
    "\n",
    "**Key features:**\n",
    "- **generator.eval()**: Sets Generator to evaluation mode (disables dropout, etc.)\n",
    "- **torch.no_grad()**: Disables gradient computation for efficiency (we're not training here)\n",
    "- **save_image()**: Saves a grid of generated images to a PNG file\n",
    "- **normalize=True, range=(-1,1)**: Converts from [-1, 1] to [0, 1] for proper display\n",
    "\n",
    "**Why is this important?**\n",
    "Visual inspection is crucial for GANs. We can see if the Generator is improving, mode collapsing, or failing to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(generator, discriminator, dataloader, epoch):\n",
    "    \"\"\"\n",
    "    Train both Generator and Discriminator for one complete epoch.\n",
    "    \n",
    "    Args:\n",
    "        generator: Generator network\n",
    "        discriminator: Discriminator network\n",
    "        dataloader: DataLoader containing training images\n",
    "        epoch: Current epoch number\n",
    "    \n",
    "    Returns:\n",
    "        Final discriminator and generator loss values for the epoch\n",
    "    \"\"\"\n",
    "    for i, (imgs, _) in enumerate(dataloader):  # Iterate through batches (ignore labels)\n",
    "        imgs = imgs.to(device)  # Move images to GPU/CPU\n",
    "        \n",
    "        # Create label tensors for real and fake images\n",
    "        real = torch.ones((imgs.size(0), 1), device=device)   # Label 1 for real images\n",
    "        fake = torch.zeros((imgs.size(0), 1), device=device)  # Label 0 for fake images\n",
    "\n",
    "        # ==================== TRAIN GENERATOR ====================\n",
    "        optimizer_G.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Generate fake images from random noise\n",
    "        z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Calculate Generator loss: how well it fools the Discriminator\n",
    "        # We want Discriminator to output 1 (real) for generated images\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), real)\n",
    "        \n",
    "        g_loss.backward()  # Backpropagate gradients\n",
    "        optimizer_G.step()  # Update Generator weights\n",
    "\n",
    "        # ==================== TRAIN DISCRIMINATOR ====================\n",
    "        optimizer_D.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Loss on real images: should output 1\n",
    "        real_loss = adversarial_loss(discriminator(imgs), real)\n",
    "        \n",
    "        # Loss on fake images: should output 0\n",
    "        # .detach() prevents gradients from flowing back to Generator\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        \n",
    "        # Total discriminator loss: average of real and fake losses\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()  # Backpropagate gradients\n",
    "        optimizer_D.step()  # Update Discriminator weights\n",
    "\n",
    "        # Print progress every 200 batches\n",
    "        if i % 200 == 0:\n",
    "            print(f'Epoch {epoch} [Batch {i}/{len(dataloader)}] '\n",
    "                  f'D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}')\n",
    "\n",
    "    # Save sample images at the end of each epoch\n",
    "    save_sample(generator, epoch)\n",
    "    return d_loss.item(), g_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Function (One Epoch)\n",
    "\n",
    "**What happens in one training epoch?**\n",
    "An epoch is one complete pass through the entire dataset. For each batch of real images, we:\n",
    "\n",
    "**Step 1: Train Generator**\n",
    "- Create random noise → Generate fake images\n",
    "- Pass fake images through Discriminator\n",
    "- **Goal**: Maximize Discriminator's output (make it think fake images are real)\n",
    "- Calculate loss and update Generator weights\n",
    "\n",
    "**Step 2: Train Discriminator**\n",
    "- Pass real images through Discriminator → should output ~1 (real)\n",
    "- Pass fake images through Discriminator → should output ~0 (fake)\n",
    "- **Goal**: Correctly classify both real and fake images\n",
    "- Calculate combined loss and update Discriminator weights\n",
    "\n",
    "**Key Points:**\n",
    "- `.detach()`: Detaches fake images from Generator's computation graph when training Discriminator (prevents Generator from being updated)\n",
    "- Separate optimizers allow independent weight updates\n",
    "- The adversarial game: Generator wants high Discriminator output, Discriminator wants low output for fakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop: Train for specified number of epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train both networks for one complete pass through the dataset\n",
    "    d_loss_val, g_loss_val = train_one_epoch(generator, discriminator, dataloader, epoch)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'Finished epoch {epoch}: D_loss={d_loss_val:.4f} G_loss={g_loss_val:.4f}')\n",
    "\n",
    "# Save the trained model weights to disk\n",
    "torch.save(generator.state_dict(), 'generator_final.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_final.pth')\n",
    "print('Saved model weights: generator_final.pth, discriminator_final.pth')\n",
    "\n",
    "# To load models later, use:\n",
    "# generator.load_state_dict(torch.load('generator_final.pth'))\n",
    "# discriminator.load_state_dict(torch.load('discriminator_final.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "**What did we accomplish?**\n",
    "✅ Built a complete GAN from scratch with Generator and Discriminator networks\n",
    "✅ Trained on MNIST dataset to generate handwritten digits\n",
    "✅ Saved generated images to visualize training progress\n",
    "✅ Saved trained models for future use\n",
    "\n",
    "**Key Concepts Learned:**\n",
    "- **Adversarial Training**: Two networks competing against each other\n",
    "- **Generator**: Creates fake data from random noise\n",
    "- **Discriminator**: Distinguishes real from fake data\n",
    "- **The GAN Game**: Generator improves by fooling Discriminator, Discriminator improves by detecting fakes\n",
    "\n",
    "**What can you do next?**\n",
    "1. **Examine generated images** in the `generated_images/` folder to see the progression\n",
    "2. **Experiment with hyperparameters**: Try different learning rates, architectures, or latent dimensions\n",
    "3. **Generate new images**: Load the saved model and create new digits\n",
    "4. **Try different datasets**: Apply this same architecture to Fashion-MNIST or other datasets\n",
    "5. **Explore advanced GANs**: DCGAN (convolutional), StyleGAN, CycleGAN, etc.\n",
    "\n",
    "**Common Issues & Tips:**\n",
    "- **Mode collapse**: Generator produces same images → lower learning rate or try different architecture\n",
    "- **Training instability**: Loss oscillates wildly → adjust learning rates or use gradient penalties\n",
    "- **Poor quality**: Images stay blurry → train longer or use deeper networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Loop\n",
    "\n",
    "**What happens here?**\n",
    "This is where the actual training occurs. We loop through all epochs and train both networks.\n",
    "\n",
    "**Training Process:**\n",
    "1. For each epoch (1 to 50):\n",
    "   - Call `train_one_epoch()` which processes all batches\n",
    "   - Both Generator and Discriminator are updated multiple times\n",
    "   - Sample images are saved to track progress\n",
    "2. After all epochs:\n",
    "   - Save trained model weights to disk\n",
    "   - These can be loaded later to generate new images without retraining\n",
    "\n",
    "**What to expect:**\n",
    "- **Early epochs**: Generated images look like random noise\n",
    "- **Middle epochs**: Recognizable digit shapes start to appear\n",
    "- **Later epochs**: Clear, realistic-looking handwritten digits\n",
    "- **Loss values**: D_loss should stay around 0.5-0.7 (not too high/low), G_loss may fluctuate\n",
    "\n",
    "**Time estimate:** On GPU: ~5-10 minutes, On CPU: ~30-60 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

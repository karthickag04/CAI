{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-11.0.0-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dell\\.conda\\envs\\text_env\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cffi-1.17.1-cp39-cp39-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.5/5.6 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading pillow-11.0.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.2/3.2 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp39-cp39-win_amd64.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, Pillow, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-11.0.0 cffi-1.17.1 cryptography-44.0.0 pdfminer.six-20231228 pdfplumber-0.11.4 pycparser-2.22 pypdfium2-4.30.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to extracted_resume_data_nltk.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Ensure NLTK corpora are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_name(text):\n",
    "    \"\"\"Extract name using Named Entity Recognition.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        chunks = ne_chunk(tags)\n",
    "        for chunk in chunks:\n",
    "            if isinstance(chunk, Tree) and chunk.label() == 'PERSON':\n",
    "                return \" \".join(c[0] for c in chunk)\n",
    "    return \"Not Found\"\n",
    "\n",
    "def extract_email(text):\n",
    "    \"\"\"Extract email using regex.\"\"\"\n",
    "    email_match = re.search(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', text)\n",
    "    return email_match.group(0) if email_match else \"Not Found\"\n",
    "\n",
    "def extract_qualification(text):\n",
    "    \"\"\"Extract qualifications by matching common degree terms.\"\"\"\n",
    "    qualifications = re.findall(r'\\b(B(?:\\.|achelor)?|M(?:\\.|aster)?|Ph\\.?D|Diploma|High School|HSC|UG|PG|CS|Engineering|Science)\\b', text, re.IGNORECASE)\n",
    "    return \", \".join(set(qualifications)) if qualifications else \"Not Found\"\n",
    "\n",
    "def extract_resume_details_nltk(file_path):\n",
    "    \"\"\"Extract details using pdfplumber and NLTK.\"\"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    name = extract_name(text)\n",
    "    email = extract_email(text)\n",
    "    qualification = extract_qualification(text)\n",
    "    \n",
    "    return {\"Name\": name, \"Qualification\": qualification, \"Email\": email}\n",
    "\n",
    "# Process all resumes\n",
    "resume_files = [\"Resume01.pdf\",\"Resume02.pdf\"]  # List of resume file paths\n",
    "resume_data = []\n",
    "\n",
    "for file in resume_files:\n",
    "    details = extract_resume_details_nltk(file)\n",
    "    resume_data.append(details)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(resume_data)\n",
    "output_path = \"extracted_resume_data_nltk.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
